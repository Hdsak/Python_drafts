{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DL_Lab_3_2_homework.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOqpfpFheJgO"
   },
   "source": [
    "# DL Lab 3.2 - Homework - Word Embeddings\n",
    "\n",
    "Welcome to the DL Lab! In this lab, you will train a **word embedding** from scratch and investigate its interesting properties.\n",
    "\n",
    "In detail, you will train an embedding on a small dataset for sentiment classification on movie reviews, play with it, and create a projected visualization of the embedding space. \n",
    "\n",
    "***\n",
    "\n",
    "**After completing this homework you will be able to**\n",
    "\n",
    "- Use **Tokenizers** for **word vectorization**.\n",
    "- Train models containing **word embeddings**.\n",
    "- Compute **word similarities** and retrieve similar and analogous words.\n",
    "\n",
    "***\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- You'll be using Python 3 in the iPython based Google Colaboratory\n",
    "- Lines encapsulated in \"<font color='green'>`### START YOUR CODE HERE ###`</font>\" and \"<font color='green'>`### END YOUR CODE HERE ###`</font>\" denote the code fragments to be completed by you.\n",
    "- There's no need to write any other code.\n",
    "- After writing your code, you can run the cell by either pressing `SHIFT`+`ENTER` or by clicking on the play symbol on the left side of the cell.\n",
    "- We may specify \"<font color='green'>`(≈ X LOC)`</font>\" to tell you about how many lines of code you need to write. This is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
    "- If you get stuck, check your Lecture and Lab notes and use the [discussion forum](https://moodle2.tu-ilmenau.de/mod/forum/view.php?id=102963) in Moodle.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-hZtgVAlaXg"
   },
   "source": [
    "# 0 - Test for GPU\n",
    "Execute the code below for printing the TF version and testing for GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wqn8Vx9KubaY",
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d813f7af-1006-487b-cfcd-b0c369ddc380"
   },
   "source": [
    "#@title Print TF version and GPU stats\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "   raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n",
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.0\n",
      "Found GPU at: /device:GPU:0\n",
      "\n",
      "Wed Dec 16 17:25:56 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   72C    P0    76W / 149W |    134MiB / 11441MiB |     11%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtDe8fNagLif"
   },
   "source": [
    "# 1 - Representing Text as Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc1yhgTSgUXv"
   },
   "source": [
    "## 1.1 - One-Hot Encodings\n",
    "\n",
    "Categorical inputs to a neural network can be represented using **One-Hot Encodings**. We already used them (quite often actually) for class labels.\n",
    "\n",
    "E.g., consider we are classifying `['bird', 'cat', 'dog', 'fish']` animals with a neural network. The one-hot encoded label of class `'cat'` will look like `[0,1,0,0]`.\n",
    "\n",
    "We might generalize this idea for creating text representations: We create a **vocabulary** of all unique words of a text. For each word, we will then create a zero vector with length equal to the vocabulary size and place a `1` in the index corresponding to the word in the vocabulary.\n",
    "\n",
    "There are several problems with this approach: It is extremely inefficient since the one-hot encoded vectors are sparse. As the vocabulary size grows large, most of the space is wasted just for storing zeros. \n",
    "In addition, one-hot vectors do not allow computation of similarity measures, as all words will have the same distance to each other.\n",
    "\n",
    "## 1.2 - Index Encodings\n",
    "\n",
    "Another approach might be to encode each word using a unique number, e.g., the **index** of the word within the **vocabulary**. This approach would be more efficient compared to sparse one-hot encoded vectors.\n",
    "\n",
    "However, as the integer-encoding is arbitrary, there is no sorting or ordering. There is still no way to represent that cats might be more similar to dogs than to fish. As there is no relationship between the similarity of any two words and the similarity of their encodings, this also confuses the neural network which might not be able to learn anything meaningful from such encodings.\n",
    "\n",
    "## 1.3 - Word Embeddings\n",
    "\n",
    "Word embeddings give us a way to use an efficient, **dense representation** in which **similar words** have a **similar encoding**. Importantly, we do not have to specify this encoding by hand! \n",
    "\n",
    "An embedding is a dense vector of floating point values. The length of the vector is a parameter you specify. The values of the embedding are trainable parameters, i.e., weights learned by the model during training. It is common to see word embeddings that are 8-dimensional for small datasets, up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but requires more data to learn.\n",
    "\n",
    "An intuitive way to think of an embedding is as lookup table. After the embeddings weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZPW2zIrqG6l"
   },
   "source": [
    "# 2 - Using the Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyCczK9oqJ-m"
   },
   "source": [
    "So much for the motivation. Let's get started and use embeddings!\n",
    "\n",
    "Keras makes it easy to use word embeddings by means of its [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
    "\n",
    "The first argument `input_dim` specifies the size of the vocabulary. The second argument `output_dim` is the dimensionality of the embeddings, hence the length of the dense vectors. The `output_dim` is a parameter you can tune and experiment with in the same way you would experiment with the number of neurons in a dense layer.\n",
    "\n",
    "**Task**: Initialize a layer for embedding words of a vocabulary of 1000 words into 5 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t3WvHkj1bLkl"
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W0ev7Swrr4ds"
   },
   "source": [
    "### START YOUR CODE HERE ###  (≈1 LOC)\n",
    "embedding_layer = layers.Embedding(1000,5)\n",
    "### END YOUR CODE HERE ###"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43EaPxOqvZdQ"
   },
   "source": [
    "When you create an `Embedding` layer, the weights for the embedding are randomly initialized just as for any other layer. During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words as learned for the specific problem your model was trained on.\n",
    "\n",
    "The `Embedding` layer can be understood as a lookup table that maps from integer indices (denoting specific words) to dense vectors (their embeddings). Hence, passing a list of integers to an embedding layer, the result replaces each integer with its corresponding vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a2RU4rWdsq9I",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b216d4ec-0a80-4179-e510-69e6b81d2451"
   },
   "source": [
    "result = embedding_layer( np.array([3, 1, 4]) )\n",
    "print(result.numpy())"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[ 0.02208264  0.01942709  0.04027826 -0.00572944  0.03155648]\n",
      " [ 0.03049347 -0.00843795  0.00522105 -0.02951161  0.03331155]\n",
      " [-0.03370769 -0.00104406  0.00360634 -0.02160817  0.01879874]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AKzT5WlwKOi"
   },
   "source": [
    "For text or sequence problems, the `Embedding` layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes `(32, 10)` (a batch of 32 sequences of length 10), or `(64, 15)` (batch of 64 sequences of length 15):\n",
    "\n",
    "**Task**: Create an input array of shape `(64, 15)` filled with `0` of type `int`, feed it to the `Embedding` layer and print the shape of the result."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IdpAsxwMuQ_r",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "223154db-952b-4d9a-d415-8621684e9318"
   },
   "source": [
    "### START YOUR CODE HERE ###  (≈3 LOC)\n",
    "arr = np.zeros((64,14), dtype=np.int)\n",
    "result = embedding_layer(arr)\n",
    "print(result.numpy())\n",
    "\n",
    "### END YOUR CODE HERE ###"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[[-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  ...\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]]\n",
      "\n",
      " [[-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  ...\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]]\n",
      "\n",
      " [[-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  ...\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  ...\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]]\n",
      "\n",
      " [[-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  ...\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]]\n",
      "\n",
      " [[-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  ...\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]\n",
      "  [-0.0310155  -0.0426484   0.04302492  0.04389408  0.0055267 ]]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgAOZAR5x7fd"
   },
   "source": [
    "The returned tensor has one more axis than the input and the embedding vectors are aligned along the new last axis. Hence, the shape of the embedded tensor is `(samples, sequence_length, embedding_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK36AOLAz-WS"
   },
   "source": [
    "# 3 - Training Embeddings from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHWgPBaS5_di"
   },
   "source": [
    "## 3.1 - Next Word Prediction\n",
    "You can actually train word embeddings in a model for solving a certain task, just as next word prediction. For this task, you will use the same text data as for the DL Lab 3.1, i.e., a collection of certain tweets.\n",
    "\n",
    "Execute the cell below for downloading and preprocessing the text data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "nfKpRufwo_nG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "91c8e15e-ebf3-41c4-cbe1-d83ef8c82674"
   },
   "source": [
    "#@title Dataset Download and Preprocessing\n",
    "\n",
    "import requests, os, zipfile\n",
    "import numpy as np\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    \n",
    "dataset_file = '/tmp/tweets.csv'\n",
    "download_file_from_google_drive('1HE_ezCVWG7JSy5fDSASAmrkmUUbPpWAC', dataset_file)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv(dataset_file, )\n",
    "tweets.head()\n",
    "\n",
    "print('Initial amount of Tweets:',len(tweets))\n",
    "# remove retweets\n",
    "tweets.drop(tweets[tweets.isRetweet == 't'].index, inplace=True)\n",
    "# remove unpopular tweets\n",
    "tweets.drop(tweets[tweets.favorites < 1e4].index, inplace=True)\n",
    "# convert to lowercase\n",
    "tweets.text = tweets.text.str.lower()\n",
    "# remove urls from tweets\n",
    "tweets.text.replace(regex=r'https?:\\/\\/(www\\.)?[-a-z0-9@:%._\\+~#=]{1,256}\\.[a-z0-9()]{1,6}\\b([-a-z0-9()@:%_\\+.~#?&\\/=]*)', value='', inplace=True)\n",
    "\n",
    "# remove hashtags and replace ampersands\n",
    "tweets.text.replace(regex=r'#\\w+', value='', inplace=True)\n",
    "tweets.text.replace('&amp;', '&', inplace=True, regex=True)\n",
    "\n",
    "# remove citation tweets\n",
    "tweets.drop(tweets[tweets.text.str.startswith('\"\"\"')].index, inplace=True)\n",
    "tweets.drop(tweets[tweets.text.str.startswith('\"via ')].index, inplace=True)\n",
    "# remove non alphanumeric chars\n",
    "tweets.text.replace(regex=r'[^a-z0-9,. ]', value='', inplace=True)\n",
    "tweets.drop(tweets[tweets.text.str.startswith('    ')].index, inplace=True)\n",
    "# remove short tweets\n",
    "tweets.drop(tweets[tweets.text.map(len) < 50].index, inplace=True)\n",
    "print('Remaining Tweets:',len(tweets))\n",
    "\n",
    "tweets = tweets.text.to_list()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Initial amount of Tweets: 55090\n",
      "Remaining Tweets: 14113\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KIk04MNdqOtD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e68bed7c-8ce4-4c82-d540-ace68266eae6"
   },
   "source": [
    "# no. of unique words\n",
    "len(list(set(' '.join(tweets).split(' '))))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26673"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbH7X18NIm8z"
   },
   "source": [
    "You can see that this dataset has more than 26k different words. However, we will use a vocabulary of 10k most frequent words as detailed in the video.\n",
    "\n",
    "TensorFlow Keras provides a convenient class for converting text from words into indexed representations: the [Tokenizer class](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer?hl=en).\n",
    "\n",
    "**Task**: (1) Initialize an instance `tokenizer` of the Tokenizer class using\n",
    "- a vocabulary size of 10`000 words\n",
    "- `<unk>` as replacement for any word not contained within the vocabulary,\n",
    "- a filter for removing special characters '!\"“#$%&()*+.,-/:;=?@[\\]^_`{|}~ \\'.\n",
    "\n",
    "(2) Next, compute the most 10k frequent words by calling the [`fit_on_texts` method](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer?hl=en#fit_on_texts) on the tweets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1unmh3ZOpNTG"
   },
   "source": [
    "VOCABULARY_SIZE = int(1e4)\n",
    "\n",
    "### START YOUR CODE HERE ###  (≈2 LOC)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    num_words = 1000,\n",
    "    oov_token = \"<unk>\",\n",
    "    filters = '!\"“#$%&()*+.,-/:;=?@[]^_`{|}~ '\n",
    ")\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "### END YOUR CODE HERE ###"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSUI9rdQL7tn"
   },
   "source": [
    "We also add another token `<pad>` that will be used for padding all sequences to equal length."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6xFLj9Aupa_K",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cf6e0259-a55e-444e-9f6e-f363786796da"
   },
   "source": [
    "# add `'<pad>'` token to tokenizer at index `0`\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# print first 10 words\n",
    "print(list(tokenizer.index_word.items())[:10])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[(1, '<unk>'), (2, 'the'), (3, 'to'), (4, 'and'), (5, 'of'), (6, 'a'), (7, 'in'), (8, 'is'), (9, 'for'), (10, 'i')]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JkRaq3yLAtI"
   },
   "source": [
    "The [`text_to_sequences` method](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer?hl=en#texts_to_sequences) of your tokenizer can now be used for converting the text data into index representation.\n",
    "\n",
    "**Task**: Convert the tweets into index representation and pad the sequences by adding `0`s at the end of every sequence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DAVVhojYtbEU"
   },
   "source": [
    "### START YOUR CODE HERE ###  (≈2 LOC)\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "sequences=tf.keras.preprocessing.sequence.pad_sequences(sequences=sequences,padding='post')\n",
    "\n",
    "### END YOUR CODE HERE ###"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oUl77goQtbG_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e4185d1f-3277-48d8-941f-d065e7a6bfbb"
   },
   "source": [
    "sequence_length = sequences.shape[1]  \n",
    "print('Max sequence length:', sequence_length)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Max sequence length: 61\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxz6w5MiNRGR"
   },
   "source": [
    "Next, we initialize the dataset, split each tweet into an input and an output sequence, and create shuffled batches:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bokbiU11xTXy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b4d8789f-4da1-4190-8f00-a60d54e6a3bc"
   },
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "def split_input_target(sequence):\n",
    "  input_sequence = sequence[:-1]\n",
    "  target_sequence = sequence[1:]\n",
    "  return input_sequence, target_sequence\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 60), (64, 60)), types: (tf.int32, tf.int32)>\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovrCmZT2N-VQ"
   },
   "source": [
    "We will use a RNN with GRU layer for next word prediction. The next cells build the model and train it for 50 epochs.\n",
    "\n",
    "If you are interested, increase the size of the GRU layer, e.g., to 1024, and train it for 200 epochs with an early stopping callback."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VqsWt7j3yTva"
   },
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model_gru(vocabulary_size, embedding_dim, batch_size, sequence_length, summary=True):\n",
    "  \n",
    "  inputs = layers.Input(batch_input_shape=(batch_size,))\n",
    "  \n",
    "  x = layers.Embedding(vocabulary_size,\n",
    "                       embedding_dim, m)(inputs)\n",
    "  x = layers.GRU(128,\n",
    "                 return_sequences=True,\n",
    "                 stateful=True,\n",
    "                 )(x)\n",
    "  x = layers.Dropout(.2)(x)\n",
    "  out = layers.Dense(vocabulary_size, \n",
    "                     activation='softmax'\n",
    "                     )(x)\n",
    "\n",
    "  model = Model(inputs, out, name='next_character_prediction')\n",
    "  model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=Adam(lr=1e-3),\n",
    "                metrics='accuracy')\n",
    "  \n",
    "  if summary:\n",
    "    print(model.summary())\n",
    "  \n",
    "  return model\n",
    "\n",
    "EarlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                         patience=5, \n",
    "                                                         restore_best_weights=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "tEJecYKv-QJ6"
   },
   "source": [
    "#@title `plot_history()` definition\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_history(history, ylim=None):\n",
    "  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n",
    "  ax1.plot(np.array(history.epoch)+1, history.history['loss'])\n",
    "  ax1.set_ylabel('Cross-Entropy Loss')\n",
    "  ax1.set_yscale('log')\n",
    "  ax1.grid(alpha=.5)\n",
    "  if history.history.__contains__('lr'):\n",
    "    ax1b = ax1.twinx()\n",
    "    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n",
    "    ax1b.set_yscale('log')\n",
    "    ax1b.set_ylabel('Learning Rate', color='g')\n",
    "\n",
    "  ax2.plot(np.array(history.epoch)+1, history.history['accuracy'])\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_xlabel('Epochs')\n",
    "  ax2.grid(alpha=.5)\n",
    "  if ylim:\n",
    "    ax2.set_ylim(ylim)\n",
    "  #ax2.legend()\n",
    "  plt.show() "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpaQq1jtyT0S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "61f93363-c1d8-4ee2-afce-68778b99b5be"
   },
   "source": [
    "gru_model = build_model_gru(VOCABULARY_SIZE, EMBEDDING_DIM, BATCH_SIZE, sequence_length)\n",
    "gru_history = gru_model.fit(dataset, \n",
    "                            epochs=50,\n",
    "                            callbacks=[EarlyStoppingCallback]\n",
    "                            )\n",
    "plot_history(gru_history)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"next_character_prediction\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(64, 60)]                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (64, 60, 16)              160000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, 60, 128)             56064     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, 60, 128)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, 60, 10000)           1290000   \n",
      "=================================================================\n",
      "Total params: 1,506,064\n",
      "Trainable params: 1,506,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "220/220 [==============================] - 29s 130ms/step - loss: 3.9254 - accuracy: 0.5023\n",
      "Epoch 2/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 3.2587 - accuracy: 0.5054\n",
      "Epoch 3/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 3.2566 - accuracy: 0.5055\n",
      "Epoch 4/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 3.2559 - accuracy: 0.5055\n",
      "Epoch 5/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 3.0560 - accuracy: 0.5249\n",
      "Epoch 6/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.5798 - accuracy: 0.5844\n",
      "Epoch 7/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 2.4982 - accuracy: 0.5853\n",
      "Epoch 8/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.4514 - accuracy: 0.5865\n",
      "Epoch 9/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.3856 - accuracy: 0.5905\n",
      "Epoch 10/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.3271 - accuracy: 0.5954\n",
      "Epoch 11/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.2819 - accuracy: 0.5986\n",
      "Epoch 12/50\n",
      "220/220 [==============================] - 29s 130ms/step - loss: 2.2463 - accuracy: 0.6013\n",
      "Epoch 13/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.2166 - accuracy: 0.6030\n",
      "Epoch 14/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.1924 - accuracy: 0.6048\n",
      "Epoch 15/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.1694 - accuracy: 0.6067\n",
      "Epoch 16/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.1480 - accuracy: 0.6087\n",
      "Epoch 17/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.1295 - accuracy: 0.6105\n",
      "Epoch 18/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 2.1124 - accuracy: 0.6121\n",
      "Epoch 19/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.0958 - accuracy: 0.6136\n",
      "Epoch 20/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 2.0803 - accuracy: 0.6152\n",
      "Epoch 21/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.0690 - accuracy: 0.6160\n",
      "Epoch 22/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 2.0564 - accuracy: 0.6171\n",
      "Epoch 23/50\n",
      "220/220 [==============================] - 29s 130ms/step - loss: 2.0446 - accuracy: 0.6180\n",
      "Epoch 24/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 2.0346 - accuracy: 0.6186\n",
      "Epoch 25/50\n",
      "220/220 [==============================] - 28s 127ms/step - loss: 2.0244 - accuracy: 0.6196\n",
      "Epoch 26/50\n",
      "220/220 [==============================] - 28s 127ms/step - loss: 2.0161 - accuracy: 0.6202\n",
      "Epoch 27/50\n",
      "220/220 [==============================] - 28s 127ms/step - loss: 2.0078 - accuracy: 0.6209\n",
      "Epoch 28/50\n",
      "220/220 [==============================] - 28s 127ms/step - loss: 1.9998 - accuracy: 0.6213\n",
      "Epoch 29/50\n",
      "220/220 [==============================] - 28s 127ms/step - loss: 1.9923 - accuracy: 0.6222\n",
      "Epoch 30/50\n",
      "220/220 [==============================] - 28s 127ms/step - loss: 1.9871 - accuracy: 0.6229\n",
      "Epoch 31/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9804 - accuracy: 0.6230\n",
      "Epoch 32/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9742 - accuracy: 0.6237\n",
      "Epoch 33/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9681 - accuracy: 0.6239\n",
      "Epoch 34/50\n",
      "220/220 [==============================] - 29s 130ms/step - loss: 1.9640 - accuracy: 0.6241\n",
      "Epoch 35/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9598 - accuracy: 0.6245\n",
      "Epoch 36/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9546 - accuracy: 0.6249\n",
      "Epoch 37/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9491 - accuracy: 0.6255\n",
      "Epoch 38/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9456 - accuracy: 0.6257\n",
      "Epoch 39/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9402 - accuracy: 0.6261\n",
      "Epoch 40/50\n",
      "220/220 [==============================] - 29s 130ms/step - loss: 1.9358 - accuracy: 0.6265\n",
      "Epoch 41/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9324 - accuracy: 0.6267\n",
      "Epoch 42/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9293 - accuracy: 0.6269\n",
      "Epoch 43/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9236 - accuracy: 0.6276\n",
      "Epoch 44/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9219 - accuracy: 0.6274\n",
      "Epoch 45/50\n",
      "220/220 [==============================] - 28s 130ms/step - loss: 1.9170 - accuracy: 0.6279\n",
      "Epoch 46/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9151 - accuracy: 0.6281\n",
      "Epoch 47/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9122 - accuracy: 0.6284\n",
      "Epoch 48/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9083 - accuracy: 0.6287\n",
      "Epoch 49/50\n",
      "220/220 [==============================] - 28s 128ms/step - loss: 1.9057 - accuracy: 0.6290\n",
      "Epoch 50/50\n",
      "220/220 [==============================] - 28s 129ms/step - loss: 1.9051 - accuracy: 0.6290\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0sAAAIoCAYAAAC4U3A2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAXEQAAFxEByibzPwAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxddZ3/8dfn3ux7mzbd95aWQtkKglD2VaCoCOM+7o4z6KioqDOyzMiM44aIqMw4g6j8VEAQBFR2sEVQKEuFltKW7nvaJs2em3s/vz/Oube3IUnT5KS3Sd7Px+M8zj3f8z3n+w0Ekk++3+/na+6OiIiIiIiI7CuW6w6IiIiIiIgcihQsiYiIiIiIdEHBkoiIiIiISBcULImIiIiIiHRBwZKIiIiIiEgXFCyJiIiIiIh0QcGSiIiIiIhIFxQsiYiIiIiIdEHBkoiIiIiISBcULImIiIiIiHRBwZKIiIiIiEgXFCyJiIiIiIh0QcGSiIiIiIhIFxQsiYiIiIiIdEHB0iBmZseY2SIzazGzNWb26Vz3SURERERkqFCwNEiZ2WjgEWAPcDHwI+BGM/tgTjsmIiIiIjJEmLvnug/SB2Z2NfAZYKq7N4dlPwLOcffDcto5EREREZEhQCNLg9f5wO/TgVLoLmCWmU3PUZ9ERERERIaMIRMsmVm1mW03MzezVTnsx3wz+4qZ3WNmG8P+9Gr4zsyKzezfzex1M2s1s81mdquZTeii+mHAa53K0tez+/M1iIiIiIgI5OW6AxH6LjAq150ArgbefqAPmVkR8DhwErAFuA+YCnwEuNjMTnL3N7IeGQHUdXrN7qx7B9r+VqAE2HCgz4qIiIiIHKImAc3uPrYvDw+JYMnMzgY+BPwP8Mkcd+cZYCnwXHisBQp78dzXCAKlZ4Dz3L0RwMyuJAgEbwXOiL67GSWFhYXlM2bMmNuflyQSCQDy8/Mj6ZQMTvo+EH0PCOj7QPQ9ILn/Hli9ejVtbW19fn7QB0tmVgz8N7AM+A4HGCyZ2SnAOHf/TQ91yoErgevdPdnT+9z9m52e7U0fCoB02u8r0oFS+L4bzOxDwOlmNt/dl4S3dgOVnV5VlXXvQG2YMWPG3FdffbUPj+71xhvB4Nf06Vo2NZzp+0D0PSCg7wPR94Dk/nvgiCOOYNmyZX2eOTXogyXgWmA6cDqQOJAHzSwf+CUw3sw63P3eLuqUAr8HFgCbgZ/0u8dvdgpB4LPa3V/s4v5vgKOAhUA6WHodmNOpXvp6xQD0UURERERkWBnUCR7M7CjgC8BP3X3RgT7v7gngPUArcIeZXdTp/cXAAwSB0h0EU+EGwtHh+YVu7qfLj8oqewi4MOxj2mXAyk5rm0REREREpA8GbbBkZjHgfwmSHFzV1/e4+zPARUAHcLeZnR++v5AgycIZwG+BD+xvCl4/TA7PG7u5ny6fklV2C8G/vzvN7Gwz+yLwD8DXe9uomVWZ2VQzmwrkp1KpA+q0iIiIiMhQNmiDJYINWU8AvuTuO/vzInf/E3AJ4MC9ZnYhcA9wLvAg8G537+hnf3tSFp6bu7nfFJ7L0wXuvoOgfyMI+vgZ4Ep3/8UBtPs5YE14zNq5s1//GEVEREREhpTIgiUzG2Nmp5nZmE7lM8zs12b2ipn93sxOiqCtycD1wFPuflt/3wfg7o8BlxL8M3kQuBB4GHhXOF3vkOPuL7n7Ancvcvcp7v6DA3zFjcC08FhZXV0dfSdFRERERAapKEeWvgI8QVaGNjOrABYDlwNzgQuAx8xsVj/b+iFQAHyqn+/p7GGC1N1p33b3vuca7L109ruSbu6XhueGKBt19zp3X+vua4FELJabgcaOZIol63bz4vq+JPETERERERkYUf52fAawzN1fzyr7MDAG+BUwmyD9djFBUob+uJhgytotZvZk+gB+Hd6fkFXeqw2owjVQPyfIqreUvWuYTuhnX3tjfXie2M39dPm6g9CXg+ruJRs59uuP8K4f/5mbHluZ6+6IiIiIiGREmTp8AvuOysDexAmfc/da4Mb0nkERtFfVw3uKsu4V7e9FFmyG9H/A+4CnCUbALiRIK/6QmZ3dTUrvqLwcno/r5n66fOkA9iEnxlQU0dAaLAf7y5pdtHekKMgbzEvpRERERGSoiPK30nKyEhSYWRx4K7AkDJTSXqP7EZRecXfr6iBYewPBfkXp8rU9vSsMlG4hGAX7K3Chuze6+51hWSXwiJnN60+f9+NpoB6YYWbHdHH/svB8/wD2ISeOnzqCwjA4am5PaiqeiIiIiBwyogyWNrPvJqkLCLK8PdmpXh7QHmG7/XUT8EmCvYzOd/c96RvufjvwCWAk8KiZHT4QHXD3duDm8PKH4Ua4AJjZlQT7Kz3l7ku6en4wK8qPc8LUkZnrxatqe6gtIiIiInLwRBksPQMcZWafC0dhridIxd15NORwYFOE7faZmeUBMwmmwZ3r7nWd67j7rcA/EYycjevFOy8ys2fTB0EiCrLLOm9+G7oe+AtwMrDSzO4In/8usAP4aN++ykPfglmjMp8XrVSwJCIiIiKHhijXLH2DIPX2d8NrA55w9z+nK4Sbn84lWB+Uc+7eYWaXAiXuvquHereY2YPuvqEXrx0NnNhF+Ymd6nRuo9XMzgS+SrB26h3ALuA24Gp3727D2kFvwcy9wdLSjXXUtySoLM7PYY9ERERERCIMltz9VTNbAHwWGAUsAb7dqdr5BKM490bVbqc+rCUI0g7kmRagpRf1ehMoEe77dNuB9KFTX64Jj2Fj7rgKRpYWsKupnZTDM6t3csGRvUpiKCIiIiIyYCJNO+buL7j7h9z9Ine/xt0bOt3/b3c/1t0fjLJdGdxiMePkGXs3xF28akcOeyMiIiIiElCOZjkknJq1bmmx1i2JiIiIyCEgsmDJzGaZ2d+b2bRO5SeFSQ0azWxZuEZIZB8LZu1dxrV2ZzMbdjX3UFtEREREZOBFObL0BeBWIJEuMLMxwEPAW4BigtTid5hZd5uvyjA1oaqY6aMyGdN5WinERURERCTHogyWFgAvdcra9lGClNs3EARLl4ZtXhlhuzJEnJKVFW+RgiURERERybEog6VxwLpOZRcAbcB17t7u7vcS7CXUVWptGeay91v686paUinPYW9EREREZLiLMlgqApLpCzMrBE4A/uLujVn11gDjI2xXhoi3zqgmHgsyv+9uTrBsy54c90hEREREhrMog6WNwFFZ1+cQBFCPd6pXDDRF2K4MERVF+Rw9sTJzvUhZ8UREREQkh6IMlh4HZpnZjWa2EPgm4MB9nerNA3q1wasMPwuy1i1pvyURERERyaUog6VvAHXAZ4B7gbnAne7+crqCmR0BzACejrBdGUKyU4g/t3Y3rYlkD7VFRERERAZOXlQvcvf1ZnY08HFgNLAEuK1TtWMJRprujKpdGVqOnVxFaUGcpvYk7R0pnlu7i1OzAigRERERkYMlsmAJIEwbfl0P928Hbo+yTRla8uMxTppezWOvbQdg8cpaBUsiIiIikhNRTsMTicQ++y0pyYOIiIiI5EjkwZKZHWVm/21my8ysPjyWmdktZnbU/t8gw92pWfstLduyh52NbTnsjYiIiIgMV5EGS2b2WeB5gnVLc4Dy8JgDfBJ4Pqwj0q2ZNWWMqSjMXD+9emcOeyMiIiIiw1VkwZKZnQt8D2gPz8cCI4Aq4Bjgu0AbcIOZnR1VuzL0mBkLZu5dp7R4pVKIi4iIiMjBF+XI0pVAB3Ceu3/R3V9293p33+PuS939S8B5QAr4QoTtyhC0YFZ15vPilbW4ew57IyIiIiLDUZTB0luAp9z9z91VcPdngCeBEyNsV4ag7CQPm+tbWVPblMPeiIiIiMhwFGWwVAL0Zr7UjrCuSLdqyouYM7Y8c714lbLiiYiIiMjBFWWwtAF4q5l1u3dTeO+tYV2RHi1QCnERERERyaEog6X7gCnArWZW1fmmmVUAPwEmA/dG2K4MUadkpRB/dvVOOpKpHPZGRERERIabbkeB+uAbwKXA+4G3m9kfgbXhvSnABUAF8EZYV6RHJ04bSUE8RnsyRUNbBy9vrGf+lBG57paIiIiIDBORjSy5+y7gVOD3BHsrXQ58KTz+jiBQehA4zd13R9WuDF0lBXkcN2XvIOViTcUTERERkYMoypEl3H0zsNDMpgELgPHhrc3AYndfE2V7MvSdOms0z76xC4DFq3bw2XNm5bhHIiIiIjJcRBospYVBUZeBkZl9DZjm7h8biLZlaDll5ii+/dAKAF5cX0djWwdlhQPybSsiIiIiso8oEzz01kXAh3PQrgxC8yZUUlmcD0BHyvnLGztz3CMRERERGS5yESyJ9Fo8Zpw8ozpzrRTiIiIiInKwKFiSQ96CrBTi2pxWRERERA4WBUtyyMvenHbV9ka21rfmsDciIiIiMlwoWJJD3pTqUiaNLM5ca3RJRERERA4GBUsyKCyYOTrzefHKHTnsiYiIiIgMF33OwWxmk/v4aGFf25Th69RZo/jVX9cDsHjVTtwdM8txr0RERERkKOvPhjVrAe/Dc9bH52QYe+v0aszAHWob21ixrYE5Yyty3S0RERERGcL6EyytR0GPHCQjSguYN6GSpRvrAVi8slbBkoiIiIgMqD4HS+4+NcJ+iOzXgpmjMsHSopW1fPzU6TnukYiIiIgMZUrwIING9n5Lf1mzk7aOZA57IyIiIiJDnYIlGTTmTxlBUX7wLduaSPHCuroc90hEREREhjIFSzJoFObFecu06sz14lVKIS4iIiIiA0fBkgwqp87cOxVv8UptTisiIiIiA0fBkgwq2euWlm6qp665PYe9EREREZGhTMGSDCqzx5QzqqwACPZcemb1zhz3SERERESGqv7ssyRy0MVixikzR3HfS5sB+MJdL3PN714lL2bEzIjHsg4zYjEjHoN4LEbcIGaGGRjhuZvPAGaGAaPLC7ny3MMYX1Wcuy9cRERERA46BUsy6CzICpaa25M0tw98CvGlG+v47T+dQmmh/pMRERERGS4im4ZnZj81s5Oiep9Id942bxxTq0sOapuvb2vkqruX4u4HtV0RERERyZ0o/0z+IeDvzWwZ8BPgF+6+O8L3iwBQVpjHI1eezoqtDSSSKVLuJFPQkUqRSkHSnVTK6Ug5yZSH9/d+dgcHPPM5u2zf6xfX7+aeFzYB8ODSLRwzsYpPnDY9l1++iIiIiBwkUQZLHwA+AZwOfA/4LzO7G/iJu/8pwnZEyI/HOHJC5YC3894TJrF9TxuLVwVpyr/xh+UcMb6Ck7NSmIuIiIjI0BTZNDx3/6W7nwnMAr4N1APvB54ws+Vm9gUz02+YMqjkxWPc9N5jmRAmd0g5fPpXL7K5riXHPRMRERGRgRZ56nB3X+3uXwEmAZcBD7E3gNpoZr82s7OjbldkoIwsLeCWD8ynIC/4z2VXUzv/ePsSWhMDn1hCRERERHJnwPZZcvcOd7/H3S8EpgE/BAqAy4GHzWyVmX3ezA7uSn2RPpg3sZLr33Fk5vrljfVc97tXc9gjERERERloA74prZmdBXwL+HhY1AI8DUwBvgMsM7Mju3lc5JDxd8dP4v0nTs5c//q5Dfzqr+tz2CMRERERGUgDEiyZ2Rgz+4qZrQQeAd4NrAL+GRjv7qcRjDbdAkwGbhqIfohE7ZqFczl2clXm+tr7XuWlDXU57JGIiIiIDJQo91kyM3ubmd0DrAf+E5gI/Ao4zd3nufvN7r4HwN03uvsVwOPAW6Lqh8hAKsyL8+P3z2dUWQEA7ckU/3j7Emob23LcMxERERGJWpQjS2uBB4B3AOuAq4CJ7v4Bd1+8n+eKI+yHyIAaW1nED993HPGYAbClvpVP//IFOpKpHPdMRERERKIUZbA0HvgtcJ67H+bu33H3nb147lvAWRH2Y9gws2PMbJGZtZjZGjP7dK77NFycOL2af73w8Mz1s2/s4pt/fC2HPRIRERGRqEUZLE1y98vc/dEDecjdX3f3pyLsx7BgZqMJ1oPtAS4GfgTcaGYfzGnHhpGPnDKVtx8zPnP9k0VruP/lzTnskYiIiIhEKcpNabdG9S7plU8BDlzu7o+5+7eB/wGuzm23hg8z4xuXzmPO2PJM2VW/WcqKrQ057JWIiIiIRCXybHjh1LD/MbPlZlYfHsvDsuOibm8YOx/4vbs3Z5XdBcwys+k56tOwU1KQx39/cD4VRXkAtCSS/MMvnqe+JZHjnomIiIhIf0UaLJnZNcBzBHsqzQbKw2N2WPZXM7suorauNLN7zGxlGJC1mdk6M/u5mc2Loo0+9mt+mDb9HjPbaGZuZt7LZ4vN7N/N7HUzazWzzWZ2q5lN6KL6YUDnRTLp69n9+RrkwEypLuX77zkWC/I9sHZnM1fe8RKpVK/+tYuIiIjIISrK1OEfBK4j2HT2m8AxQFV4HA38F9AEXB3Rupp/Ad4G7AIeAx4EWoEPAkvM7OII2uiLq4FvAO8EugpyumRmRQRp1K8GyoD7gA3AR4AXuxgtGgF03uBnd9Y9OYjOnFPD584+LHP92Gvb+cWz63LYIxERERHpryhHlj4HJIAz3f2r7r7U3feEx9/c/V8Ist51hHX76+3ACHc/0d0vDY/ZwBVAPvC/ZpYXQTsH6hng68AlwDigtxvwfA04KXz+MHd/t7ufCHwBGA3cOgB9lQh95qyZnHN4Teb6riUbctgbEREREemvKIOlw4En3H1JdxXCe4+HdfvF3Z9299Yuyn8ErAbGAHP39x4zO8XMLttPnXIzu9bM4r3o1zfd/Rp3v7+3SS/MrABIp/2+wt0bs953A7AUON3M5mc9thuo7PSqqqx7cpDFYsaXL5iTuX5l0x627XnTt6iIiIiIDBJRBkt76N0v6fVh3YGUXl3f3lMlM8sHfgn8ysze0U2dUuD3BFMMPxphH7OdQhD4rHb3F7u4/5vwvDCr7HVgTqd66esV0XZPemtmTRmTRu7dY/mJ17bnsDciIiIi0h9RBkt/JBj9KO6uQnjvNOChCNvt3MYHCRIcrAyPbrl7AngPwVqnO8zsok7vKgYeABYAdzBwU+GODs8vdHM/XX5UVtlDwIWd/nlfBqx09zci7p/0kplx1uy9U/EeV7AkIiIiMmhFGSx9hWAk5x4zm9n5ppnNAO4O63w5qkbN7EtmdpuZ3WVmrwA/B7YA73X35P6ed/dngIsI1lLdbWbnh+8tJEiycAbwW+ADvXlfH00Ozxu7uZ8un5JVdgvBv787zexsM/si8A8E66V6xcyqzGyqmU0F8lOp1AF1Wrp21uFjMp8Xr6qlrWOgvm1EREREZCBFmQDhP4GXCBIbLDezl4B0OrApBNnxYgQjNf9p6TzLAXf3j/Wx3fOBs7Ou1wF/39Paqc7c/U9mdknYt3vN7F0EiSLOJciy92537+hj/3qjLDw3d3O/KTxndj919x1mdi5wM0EftwFXuvsvDqDdzwHXpi927tx5AI9Kd06cNpLi/DgtiSTN7Un+8sYuTjtsdK67JSIiIiIHKMpg6cNZn+PA/PDobGEXZQ70KVhy93MgGCUB5gHXAE+Z2dfc/T8O4D2PmdmlwL0EwQfAw8C7wul6hxx3f4lgimBf3QjcFn5+uLq6ela/OyUU5cdZMGsUjyzbBgRT8RQsiYiIiAw+UQZLZ0b4rgPm7nXAIjO7kDB9t5k97O7PHcBrHg6fPT28/ra79zb1d3+ks9+VdHO/NDw3RNlo+M+sDsDMErFYpHsUD2tnz6nJBEuPvbaNaxfOpdNoqoiIiIgc4iILltz9qaje1R/unjCzOwhGtRYCvQqWzCxGsN7pdIJU3XMJ1jCdc4ABV1+sD88Tu7mfLtcup4PEmXP2JnnYsKuF1TsamVlT3sMTIiIiInKoGapDCbXhuVdznyz4k///Ae8DniZI5f1+ghGdh8zs2IHoZJaXw/Nx3dxPly8d4H5IRMZUFHHkhIrM9WPLlRVPREREZLCJPFgyswIze6+Z/cjM7guPH4VlBVG31430NLrV+6sYBkq3EKy5+itwobs3uvudYVkl8IiZzRuYrgJBgFYPzDCzY7q4n9409/4B7INETCnERURERAa3SIMlMzuFYG+j24FPEUyDWxh+vh1YaWYnR9GOmV0QTp3LLs83s88AHwRaCPZG2p+bgE8S7GV0vrtnNsx199uBTwAjgUfN7PD+9r0r7t5OkNUO4IfhRrgAmNmVBPsrPXUgGf4k97JTiD+/bjf1zYdknhARERER6UZka5bM7DDgDwRpsJcQBEdrCTLdTSUIYOYDvzezE9y9xw1j92MW8FOg1syWADuBUQTZ8MYRbDL7YXffsJ8+5wEzCabBnRsmPNiHu98ajojdEL57+X7eeRFwdVZRQVj+bFbZ1939QfZ1PXAOcDJBULmIIOX6icAO4KM9tSuHnqMmVDKqrIDaxnaSKedPK3ew8Ojxue6WiIiIiPRSlCNL/0oQKH3e3U9w9++7+33u/jt3v8ndTyDY16cirNsfTxHs67SCYNTlcoJ1RruAHwDzwml0PQr3TroUONvdd/VQ7xZgtrs/3ou+jSYIcNJHOgVadtmb1lK5eytBRsGvE+y39A6CYOk24Dh3f6MXbcshJBYzztBUPBEREZFBK8rU4WcDL7r797ur4O43mdmHCEZQ+szd19D/gCv9rhaCKXv7q9fjKFVWvdvYu3dRX/pyTXjIEHDWnBp+s2QjAE+u2E4y5cRjSiEuIiIiMhhEObI0GnitF/VeI5gyJzLknTprFHlhcLS7OcFLG3bnuEciIiIi0ltRBks7gdm9qHcYwXQ5kSGvvCift0wbmbnWVDwRERGRwSPKYOkJ4Fgz+1R3FczsEwRJHnqz9kdkSDgra4Na7bckIiIiMnhEGSxdT5CF7odmtsjM/tHM3hYenzKzpwj2M2oG/iPCdkUOadnB0mtbG9hUt98lciIiIiJyCIgswYO7LzezS4D/R5CZrvN+SgZsA97v7j2m3xYZSqaPLmPaqFLW1DYB8MRr2/nASVNy3CsRERER2Z8os+Hh7o+Z2XTg74BTgfSmMpuBRcCd7t4cZZsig8GZs2tYU7sGCNYtKVgSEREROfRFuSntDcBud/86Qers26J6t8hgd/bhNdz6dBAsPb2qlpb2JMUF8Rz3SkRERER6EuWapU8TbBArIp2cMHUkZYXB3ybaOlI880ZtjnskIiIiIvsTZbC0MeL3iQwZBXkxTp21d3sxpRAXEREROfRFGdzcC5xuZuURvlNkyMjOivf48u24ew57IyIiIiL7E2WwdC2wHvi9mR0b4XtFhoQzZu8NljbXt7JiW0MOeyMiIiIi+xNlNrz7gDaCtOHPm9kWguCptYu67u5nR9i2yCFvdHkhR0+q4uUNdUCwQe2csRU57pWIiIiIdCfKYOmMrM9GkDZ8fNdV0fwjGZbOml2TCZaeeG07V5w5M8c9EhEREZHuRBksTYvwXSJD0tmH1/C9R18H4IX1u9nd1M6I0oIc90pEREREuhJZsOTu66J6l8hQdcT4CmrKC9ne0EbK4anXd/COYyfkulsiIiIi0oXIEjyY2TVmdkkv6i00s2uialdkMDGzfbLiPaYU4iIiIiKHrCiz4V0HvKMX9S4hyJwnMiydmRUsPbViOx3JVA57IyIiIiLdycUmsnFAvx3KsLVg5igK4sF/entaO1iybneOeyQiIiIiXclFsHQEoN8OZdgqLczjxOkjM9ePayqeiIiIyCGpXwkezOzWTkULuijLbms2cDxwb3/aFRnszp5Tw6KVtUAQLH31wsNz3CMRERER6ay/2fA+nPXZgZnh0ZOlwJf62a7IoHbWnDFcd/8yAFZub2TDrmYmjSzJca9EREREJFt/g6Uzw7MBjwN/BL7ZTd12YLNSjIvA5OoSZtaUsWp7IxCMLn3o5Km57ZSIiIiI7KNfwZK7P5X+bGY/AxZll4lI986eU5MJlh5TsCQiIiJyyIkswYO7f8Tdu1uvJCKdZKcQf3b1TpraOnLYGxERERHprL/T8LpkZnlANVDYXR13Xz8QbYsMFvOnjKCiKI89rR20J1M8vaqW844Ym+tuiYiIiEgo0tThZnaOmT0JNAKbgTXdHG9E2a7IYJQfj3HaYaMz10+sUApxERERkUNJZCNLZnYx8FuCTWd3EwRFDVG9X2QoOvvwGh5YugWAx5Zvx90xsxz3SkREREQg2ml41xKMVH0euNndkxG+W2RIOv2wGszAHbY3tPHq5j0cOaEy190SEREREaKdhncE8Iy7f1+BkkjvjCwt4LjJIzLXX73nbyzfsieHPRIRERGRtCiDpUZASRtEDtBF88ZlPv9tUz0Lf7CY7z68gtaE/uYgIiIikktRBkuPAsdH+D6RYeFDJ0/lsvkTM9cdKecHj6/iopsW8dzaXTnsmYiIiMjwFmWw9GWgwsy+GaYOF5FeiMeM71x+NLd95AQmVBVnylfvaOLyW57h6ntfoaE1kcMeioiIiAxPUQY1HwH+AHwReFeYQnwjkOqirrv71yNsW2TQO2N2DQ9//jS+8/AKbvvzWtyD8l88u45Hl2/j+nccydmHj8ltJ0VERESGkSiDpesABwyYHh7dcUDBkkgnpYV5XLvwCBYePZ6v3L2U17c1ArClvpWP/ex5Fh49nmsXzmVUWbf7PYuIiIhIRKIeWRKRCBw3eQQPfOZUfvzkam5+YiWJZDDMdP/Lm1m0cgdXXzSXS4+boD2ZRERERAZQZMGSu/8sqneJCBTkxfjsObO4cN5Yvnz3Ul5YXwdAXXOCL9z1Mve+tIn/fOc8Jo0syXFPRURERIamKBM8iMgAmDWmnN986mT+7ZIjKC2IZ8oXrazl7Bue4tO/fIGnXt9BMuU57KWIiIjI0NPnkSUzOw3Y6u6vH+Bz5wBz3f2mvrYtMtzEYsaHTp7KOXPH8K+//RtPrtgBQHtHigeWbuGBpVsYV1nEpcdN4LL5k5g2qjTHPRYREREZ/PozsvQkQbrwNzGzXWb2g26eez/wvX60KzJsTagq5qcfPoHvv+cYRpfvm+RhS30rP3xiNWd+50kuv+XP3PncBhrbOnLUUxEREZHBr79rlrpbXV4F6E/bIgPAzHj7MRN425HjeGLFdu56fiNPrNi+zzS859bu5rm1u7n2d69y4VslSe8AACAASURBVLxxXH78RN4ydSSxmBJCiIiIiPSWNo8VGaQK8mKcf8RYzj9iLDsa2rj3xU3ctWRDJt04QEsiyd0vbOTuFzYyeWQJ7zpuIpcdP3GfzW9FREREpGtK8CAyBIwuL+QTp03noc+dxn1XnMIHTppMRdG+fwtZv6uZ7z36Oqd/6wmuue8VahvbctRbERERkcFBI0siQ4iZcfSkKo6eVMXXLprLI8u2cdeSjSxauQMPZ+l1pJyfP7OOu5ds5JOnzeDjp06jtFD/KxARERHpTL8hiQxRRflxFh49noVHj2dLfQv3vLCJX/5lPZvqWgBoak/yvUdf5/a/rONz58zi3cdPIi+uwWYRERGRNP1mJDIMjKss5oozZ/L4F0/n6ovnUlWSn7m3o6GNf/3tK5x/45946NWtuGu/JhERERHo/8jSAjO79QDvLehnmyLSR4V5cT62YBqXzZ/ILU+t5tbFa2jrSAGwekcT//CLJcyfMoJ/uXAO86eMzHFvRURERHKrv8HSzPA40Hv607VIDlUW5/PlC+bw92+dwg0Pv85vXtiYWdO0ZN1u3vXjZzj/iDFcdcEcZowuy21nRURERHKkP8HSRyLrhYjkxLjKYr59+dF8/NTpfPOPr/H4a9sz9x56dRuPLt/Oe06YxGfPmUVNeVEOeyoiIiJy8PU5WHL3n0XZERHJndljy7n1wyfwzOqd/NcflvPyxnoAkinn//1lPfe8sIkPnDSZT542g9HlhTnurYiIiMjBoQQPIpLx1hnV3HvFKdz8vmOZPLIkU96SSPKTRWs49VuPc/0Dy9je0JrDXoqIiIgcHAMaLJnZx3pIACEihyAz4+KjxvPoladz3cK5VJcWZO61JlL87+I1nPrNJ/j3+5exfY+CJhERERm6BnpkaQHwoQFuQ0QGQEFejA+fMo1FXz6Tf7lwDqPK9gZNbR0pbn16Dad+6wn+7f5XFTSJiIjIkKRpeCLSo5KCPD552gwWXXUWX7vocEaV7V2z1NaR4qdPr2XBt57gut+9ytZ6BU0iIiIydChYEpFeKS6I8/FTp7PoqjP52kWH75Poob0jxW1/Xstp336Ca+97hR1NiRz2VERERCQaCpZE5IBkB03XXDyXmk5B08+eWccHfrWa7y3awiub6nPYUxEREZH+6e+mtPuzE1g/wG2ISA4U5cf56IJpvO/Eyfzqr+v58ZOr2d7QBkAi5dy/vI77ly/myAkVvOeEyVxyzHgqivJz3GsRERGR3hvQkSV3/6K7TxvINkQkt4ry43zklGn86aoz+bdLjmBsxb6b176yaQ9fu/cVTvyPx/jiXS/z/NpduHuOeisiIiLSewM9spRhZqOAOnfvOFhtisjBU5Qf50MnT+XdJ0zi/x55mQdX1LFsW0vmfksiyW+WbOQ3SzYys6aM95wwiUuPm8jIrNTkIiIiIoeSyEaWzOx4M7vGzOZ2Kn+nmW0FtgE7zeyzUbUpIoeeovw4b5tTxc1vn8pDnzuNj54yjaqSfaffrdreyPUPLufE/3yUK375AotX1pJKabRJREREDi1Rjix9Bng38MN0gZlNA34N5ANbgDHADWb2srs/GWHbInIImj22nGsWzuWqC2bz8LJt3PHcep5etTNzP5F0Hly6hQeXbmHiiGIuPW4i5x8xhrnjKjCzHPZcREREJNpg6STgRXffmVX2UYJA6YvufoOZzQeeBT4LPBlh2yJyCCvKj3PJ0eO55OjxrNvZxJ3Pb+Cu5zdmEkIAbNzdwk2PreSmx1YycUQx580dy/lHjOH4qSOJxxQ4iYiIyMEXZbA0BnipU9m5QBNwM4C7LzGzRcDREbYrIoPIlOpSvnT+HD5/zmE8sWIHdzy3nsdf2072LLyNu1u49ek13Pr0GkaWFnDO4TWcN3csC2aNoig/nrvOi4iIyLASZbAUz36fmZUBxwGPu3t7Vr3NBKNQIjKM5cVjnDt3DOfOHcPW+lbufWkTD7+6lRfW1+1Tb1dTO3c+v5E7n99ISUGc0w8bzXlHjOGs2WOoLFEqchERERk4UQZL64H5WdcXhe9/tFO9CkA7VYpIxtjKIj51+gw+dfoMtu9p5ZHl23jo1W08s7qWRHLvkFNze5I/vLKVP7yylbyYcdL0as6aU8OCWaOYVVOmdU4iIiISqSiDpfuBq8zsHuAJ4CogBdzXqd6xwLoI2xWRIaSmooj3nziF9584hT2tCZ54bTsPL9vGk69tp6k9manXkXIWr6pl8apaAEaXF7Jg5igWzBzFKTNHMbayqLsmRERERHolymDpO8DfAe8ID4Ab3H1luoKZnQhMAH4VYbsiMkRVFOXz9mMm8PZjJtDWkeTPq3by8LKtPLJsG7WN7fvU3dHQxm9f3MRvX9wEwMyaskzgdOL0kVQUacqeiIiIHJjIgiV3rzWzo4DLgNHAEnd/vFO1scD3gdujaldEhofCvDhnzqnhzDk1XP8O58X1u3l0+XYWr9rBq5v34J22aVq1vZFV2xu57c9riceMoydWZoKnYyZXUZinRBEiIiLSsyhHlnD3RuC2Hu7fx5un5YmIHJB4zDh+6kiOnzoSmMOupnaeWb0znJa3gw27Wvapn0w5L6yv44X1ddz0+CoK4jGOmFDBsZNGcOzkKo6bMoLxlUVa8yQiIiL7iDRY6o6ZVQKzgI3uvvVgtCkiw8fI0gIuOmocFx01DoD1O5tZvKqWp1fV8vTqWuqaE/vUb0+meHF9HS+ur4Ong7Ka8sIgcJo8gmMnj2DehEqKCzT6JCIiMpxFFiyZ2XnAe4AfuPuLWeWfAb4FFABuZj9w989H1a6ISGeTq0t4X/Vk3nfiZFIpZ9mWPSxaGQRPz63dRVtH6k3PbG9o46FXgyx8EIxeHT6uPAyeqjhm0gimVpdo9ElERGQYiXJk6ePA24B/TheY2TzgRiAJPAscDvyzmT0ZTskTERlQsZhx5IRKjpxQyT+eMYP2jhSvbd3DC+t28+KGYHRp/a7mNz2XTDmvbNrDK5v28PNnggSelcX5HDWxkmMmVXH0xCqOnlTF6PLCg/0liYiIyEESZbB0HPBSuG4p7cPps7v/0symAcuAf0Jrl0QkBwryYhw1sYqjJlZl/gdV29gWTsvbzYvr63h5Yx3NWWnK0+pbEixaWcuilbWZsglVxUHwNKmSoydWceSESkoLD8oMZxERERlgUf5EHwM816nsbKAO+DWAu68xs6eAuRG2KyLSL6PKCjl37hjOnTsGCEaVXt/WwAth8PTi+t2s3tHU5bOb6lrYVNfCg3/bAkDM4LAx5UHgNLGSeRMqmTO2nKJ8rX8SEREZbKIMlpJAZhdIMxsJHAk84O7ZCwR2EKQWFxE5JAXrlSo4fFwF7z9xCgB7WhP8bWM9L22o4+UNdby0oY7tDW1vejbl8NrWBl7b2sAdz28AIC9mzBpTzpHjK5g3MZgSOHdchQIoERGRQ1yUwdJa4GQzy3f3BHApYMAjnepVAzsjbFdEZMBVFOVzSrhPU9rW+tYgeNpYx0vr6/jbpnoa2zre9GxHylm+ZQ/Lt+zhriUbgSAgmzm6jCMnVDJvQhBEHT6ugpICTeETERE5VET5U/kO4D+AP5nZn4GPAAng3nQFC9JIzQdWRNiuiEhOjK0s4oLKsVxw5FggmL73xo7GTAD1yqYgQOoq+14y5azY1sCKbQ3c/UJQFjOYUl3KYWPKmD22gtljypk9toyp1aXkxWMH80sTERERog2WbgLeDpwYHingS+6+KavOWQRT8G6JsF0RkUNCPJxuN2tMOZcfPwmARDLFqu2NvLKpnlc21fO3TfUs27KH1sSbA6iUw5raJtbUNmVSmAMUxGPMqClj9pgyDhtbzuwx5Rw2ppyJI4qVylxERGQARRYsuXuTmZ0MnEYQEL3k7is7VUsCnwfuj6pdEZFDWX48lln/lA6gOpIp3qht4m8bg+Dp1c31vLp5T5cZ+CDYRDc9jS9bWWEes8aUMXN0WXCuKWPm6CCIisUURImIiPRXpJPjw0QOT/Zw/8me7ouIDAd58RiHhaND75o/EQim5a3d2cTrW4Opea9va2DF1gbW1DaR8q7f09jWEWbrq9unvCg/xvRRQfA0qyYMomrKmFJdSkGepvOJiIj01oCtJDazGmBCeLnJ3bcPVFsiIoNdPGbMGF3GjNFlvG3euEx5ayLJ6h2NYfDUmAmiNtW1dPuu1kSKZVv2sKzTSFRezJhSXZIJntLtzagpo0x7Q4mIiLxJ5D8dzeyfgM8CMzuVrwS+7+4/jrpNEZGhqig/zhHjKzlifOU+5Q2tCV7f1sjKbQ2s2t7Iyu2NrNre2GMQ1ZFyVu9oYvWOfddEAYytKAoDqNJMIDWzpozR5YVaFyUiIsNWZMGSmcWAO4F3EqQMrwPWAQ5MAQ4Dbjazs4HL3b2biSUiIrI/5UX5zJ8ygvlTRuxT3tzewertTazaEQZR2xpZtaORdTubSXY3nw/YuqeVrXtaWbyqdt92CvOYXhOsi5o+upRpo4JjanUpxQXaJ0pERIa2KEeWPkmwt9IKgix4D2TfNLOLgG8TBFOfBP47wrZFRAQoKchj3sRK5k3cdySqvSPFup1NmRGo1TvCY3sTLYmuE0sANLR18HK4EW9n4yuLmDa6lOmjyoIganQp00eVMqGqOPKvS0REJBeiDJY+AuwBznD3bZ1vuvuDZraEIJj6KAqWREQOmoK8WCatebZUytmyp5XVWUFUcG6itrGtx3durm9lc30rT6/ad5/x/Lgxrjyf8RUFzBzfxLjKYsZWFDGusoix4aHNd0VEZDCI8qfVXOCRrgKlNHffamaPAedG2O6wZWbHAD8Ajge2At9195tz2ysRGUxiMWNCVTETqoo57bDR+9yrb06wasfeUai14R5Qa2ubaU++eZ+otETSWV/Xzvq6dp5d39hlncri/EzwNK6yiLEVxZnriSOKGV9VTFG+pvmJiEhuRf2nvd6sQ9JapQiY2WjgEeCvwMXAccCNZlbv7r/IaedEZEioLOl6XVQy5Wyua+GN2ibW7GhkTW1T8Lm2iU11LfRmRWp9S4L6lgSvbW3ots6oskImjihmwohiJlaF5xHFTKgqYcKIYmXwExGRARflT5oVwFlmNsrda7uqYGajgLPCutI/nyIIPC9392bgMTObBlwNKFgSkQETjxmTRpYwaWQJp3cajWpNJFm/q5lnXlnNloYEibxStta3srm+ha31rWzb09rtvlGd1Ta2UdvYxktdrJcCqCrJz4yKja8KRqbGVxUzvqqIcZXF1JQXkhfXvlIiItJ3UQZLPwO+T/BL+5Xu/lj2TTM7E7gBqABui7Dd4ep84PdhoJR2F/CPZjbd3d/IUb9EZBgryo9z2Jhy8poqAJg+ffo+9zuSKWob29kSBk9b6oMsfFvqW9lS1xKc61t6FVDVNSeoa07w6uY9Xd6Px4wx5YWMC4Op8eGUvyCwKqamopDq0gIFVCIi0q0og6UfARcAbwMeNrMdBKnDIUgdPpogpfjvw7p9ZmYlwHnAQmBB+P4ksAq4G7jB3bueKD/AzGw+wZqst4THBAB33+9GJWZWDHwVeA8wGdgF/BG42t03dap+GPBAp7LXwvNsQMGSiBxy8uKxTJKH7iSSKbbWt7KproVNu1vYuLuFTXXNmevNda09rplKS6Y8k4RiybrdXdYxg+rSQmrKC6mpCM5jKoqoKS9kdHlRpmx0eSGFeVpDJSIy3EQWLLl70swWAp8H/hmYBNRkVVlPkIzge+6+/59yPXsf8JPw83LgdwQjVicD/wa818xOd/ft/WynL64G3n6gD5lZEfA4cBKwBbgPmEqQZfBiMzup02jRCIK9rLLtzronIjIo5cdjmWl+XUmlnB2NbWEQ1cLmuha21LWwqS4YldpS38qupvZeteW+d7rfsi091x1Rkh8EUmEwNaYiHVgVZT6PLi8kXyNVIiJDRpSb0lYA7u7fBb5rZpOA8eHtze6+Iaq2gATwP8CN7r48qw/jgAeBY4EbCYKqg+0ZYCnwXHisBQp78dzXCAKlZ4Dz0iNjZnYl8F3gVuCM6LsrIjK4xGLGmIoixlQUvSn5RFpLezITOG2qa2FLGEhtCqf6batvpaGt44Da3d2cYHdzz0kpAKpLC6ipCAKoUWXBVL+R4TGqrDDzubqsQCnURUQOcVH+X7oO+AvwVoAwOIoyQMpw958RrJHqXL7FzK4A/gxcamYF7t7jnxfN7BRgnLv/poc65cCVwPXu3v3ujUEfvtnp2Z6qp+sUAJ8OL6/InkLo7jeY2YeA081svrsvCW/tBio7vaoq656IyLBVXBBn+ugypo8u67ZOS3uS7Q2tbG9oY/ueNrbtCT83tLIjXdbQSl1z4oDa3tnUzs6mdpbvZ6QKoCg/RnVpVgCVDqzKgs/VpYWZzyNLCygrzOvVzxUREYlGlMFSPYfGOpmXw3MhUE0wpa1LZpYP/BIYb2Yd7n5vF3VKCdZZLQA2s3f6X5ROIQh8Vrv7i13c/w1wFMEarXSw9Dowp1O99LWyDYqI7EdxQZwp1aVMqS7tsV5bRzIInhra2L6nlW1hYLVtTxBY9TWoAmhNpIK1WHUtvapfEI9lRqX2BleFVJcVMKJk7wjWyNJ8RpQUUFVSQDym4EpEpK+iDJZeBGZE+L6+SqdeShAkSOiWuyfM7D3Aw8AdZnapuz+Yvh8mXHiAIFC6g2Aq3EA4Ojy/0M39dPlRWWUPAZ82s2J3T/+UvQxYqUx4IiLRKcyLM3FECRNHdL2GKq01EQRV27ICqp1NbexqamdnYzu7moJjZ1M79S0HHlgBtCdTbN0TZBDsDTOoKs5nRGkBI7OCqfR1VUk+I0uDoGpE+LmiKJ+YAiwRESDaYOmbwO/N7LKeprQdBJ8Nz39097b9VXb3Z8zsIuAPwN1m9nZ3f8jMCgmSLJwB/Bb4wP6m4PXD5PC8sZv76fIpWWW3ECTSuNPMbiRYp/UPwEd726iZVbF36l5+KtXfvBsiIsNXUX68x8QU2RLJFLubwwCqMQig0oHUrjDAqs0KsHY3t/dqs9/O3PeutXqDpl49EzOoSgdSJfsGUuVFeZQWBkdZeJRmzvHMtZJciMhQEWWw1AL8L8EIzQPA/QQZ8Lr885e7/ynCtgEwswuBjxGMKl3d2+fc/U9mdgnBKNK9ZvYu4AqCFOAPAu929wNbCXxg0pPqm7u5n/4JV54ucPcdZnYucDNBH7cBV7r7gWxI+zng2vTFzp07D+BRERHpq/x4jJryIJNebyRTngmudja2v3nEqrmd3U37BleJZB+iKyDlZN7T2wCrs8K8GGWFeRTEnPLCODVVO6gsyaeyODiq0ueSfCoynwuoLM6ntCCudVkicsiIMlh6EnCCvZQWAhfvp36kG1aY2Rzg9rD9L7n7y/t5ZB/u/piZXQrcSxB8QDA9713u3rf5EgPM3V8imCLYVzeyd4Pgh6urq2f1u1MiIhK5eMwYVRZk12PM/uu7O41tHexuSrCrOT1alQgCquZgNGt3czt1zcH9uuZ2djcnSPZmN+BeaOtI0dYR5Ffa0pDg9dreTRsEyIsZFcX5VBTlhed8KorzKC8MzsF1p89Fe69LFGyJSISiDJZ+ThAsHXRmNoFg89YRBBvSfr+Pr3qYIHX36eH1t3szlS8C6ex33c3dSK8+7jlf7QFy9zrCvZrMLBGLadqEiMhQYGaUF+VTXpTP5Or9TwuEYP+qhrYOdocjU7ub29ndlNj7uTlBY2sHTW0dNLQF56a2DhrDozURzVTujpRnRrb6Ih6zNwVaFUWdPhfnU16UR0lBHsUFcUoK4hTnxzOfS/LzKCqIURCPKfASGeai3JT2w1G960CY2UiCIGcK8FPgi318T4wg4DudYJ+kuQRrmM5x9+ci6m531ofnid3cT5evG+B+iIjIMBWLWWaa3FR6zhDYlY5kiqb2JI1ZQdTKNRtobE9RWD6C+pZEcDQH57qWdupbOtjTkqCuuZ2m9miWBQdTFoN1Wv0Vjxkl+XGKsgKqkoI4JQV5FGU+ZwVZBXmZOsXhdWlBPFjXVZRHebimS6NfIoPHoN4Nz8zKCBIzzAXuAT7hfuBLYC34P9b/EWxi+zRwAXAhQVrxh8zs7G5SekclPWXwuG7up8uXDmAfRERE+iwvHqOyOEZlcX6mrKoj2PZv+vQp3T2WkUimgsCpJUFDaxBE7WlNsKelIzx3dR3Uq29J0NYRfZKiZDjadqAbGO9PzNgnSUZZUdbnrMCqrCiP8qJ8ygrzKC9KH3uvSwvylLlQZID1K1gys7MIRj2ed/dl+6k7Fzge2ODuT/Sn3fB96Wx1byFIo/3evmSrCwOlW4APA38FLgw3hb0z3Cz2Z8AjZnamu/+tv/3uxtME+1TNMLNjwrVI2S4Lz/cPUPsiIiI5lR+PUV1WSHVZYZ+eb+tIZgVZ+w+2Glo7aG5P0tIenhNJWtqTdES0bqsnKYeG1g4aWvsXhJlBWUEQVJUW5lGUH6MoL05Rfpyi/BiF+fHwOpYpK8oLRr0K8+OU5MczgVl5UX4maCsvyqMwT1MQRaAfwZKZTSJIhLABmN+LRzYQpOCeaGaz3H1zP9qOA78CzgIWAZe6e98mN8NNwCcJ9jI63933pG+4++1hwPS/wKNmdoa7L+9rv7vj7u1mdjPwr8APzew8d28CMLMrCfZXesrdl/T0HhERkeGqMC9OYVk8SILRD+0dqUzg1BwGUq2JJM3tyTCoSgdZyX2CrHTddHlzIgjEmtqSNLV30NjaEXkg5s6AjHwB5MctDJzyM6NdFUXpaYTBVMLSgjgl4bTCdFlJOO2wOD841zYlKMqP0d6RIj9uCsBk0OnPyNLHgQLgKnffb+IBd28wsy8RZJv7GPD1frT9aeCd4eda4Efd/Mf3RXev7e4lZpYHzCSYBndumPCgc79vDQOmG4BxQI/BUrhnU3ba8oKw/Nmssq9nb34buh44BzgZWGlmiwjWYZ0I7OAA9k8SERGRvinIi1GQt+90wii4O20dqSAhRuvexBiNrR00tQejTOn1XulRp8a2RKY8uyyqZBo9SSSjW/sVeJ2YEY5wxSnKC0a7CrNGvPaOgIUjY3lxCtPndP28WHCE7yjMKist3DtVUSnoJSr9CZbOBXa4+729fcDdf2dm24C30b9gaUTW53d2WwuuIwimuutPR5guvMTdd/VQ7xYze9DdN/Sib6MJApzOTuxUp3MbrWZ2JvBVgrVT7wB2EaT2vtrdu9uwVkRERA5xZpYJBKIY/coEVW3pKYVBRsLWRDJzbkkkaUskae1Il++919qRoqU9OwgLjqjSx3cl5WRG6AZazMiMjJUX5e1NQR9ep4OqorwY8XiM/JgRjxn58Vh4NuKxGHlxIy9m5GV9Li6IU1qQ3qA5TmFepLvhyCGmP8HSHIK1NgfqeYLRkz5z9+sIAqF+c/cWgg1191evN4ES7n4be/cu6ktfrgkPERERkTcJRr8KGFFaEOl73Z2WRJLG1g72tO4d+WpoTdCQNSLWnDXtMHNuS9Kc6KA5nHaYDooGMvjqScoJ1q71c11Yb+THLQicCoI1X6WF8UwCj/S5MC8dbMXIjxt58Rh5YXCWFw/O+dn3Y8EIZ2HW6FnmOi+e9VlrywZaf4KlUoKkBAeqHijrR7siIiIiEjEzC9ce5VFT0f/3uTsrVq6mtcMZO2FSOKKVNbrVebQraxSsLevclkjR1tGprCNFWyJJe8fekbKmto4ByYq4P4mkU9ecoC6yKYsHpiAvRmE8RmE4jTF737Aglf3eVPfFnfYUy57aWJAXoyAez0xFLYjH9gnK0uX58RhxM8wYFoFaf4Kl3fRqH/E3GRM+KyIiIiJDlJmFv2DD2Mqig9Jme0cqGAlr3Zv1sKE1yJCY/pwpa+mgPZmiI+V0ZJ2TKSeR9OCcCq47kk5HKkVH0jOJPQ4V7R2p4Otuy037MYNYVvAUMzAsU57yFDGDeHwVL1597qALsPoTLC0DTjKz4nD62H6ZWQnwVoIU3SIiIiIikSnI618K+t5Kppym9iAhR7AJczKzGXNXZYlkEGglwoArkUyRyArAElnBWiJzP5UZRWvvCEbXEv+fvfsOs+uq7/3//k6fUW+2bMmyLQG2MRjbNFOCDZhObsAQLkkopub+ggkJCQlcIBBwkptwMYQncCGYGriEUEy5JKaEZrAJ4IplCxsVq1iyrd6mz/f3x94zOhqfGc1oypkzer+e5zy7772OtCTNR2vttfpr061xNAMJA0PTnI5cvoiBugtKMLGw9P+AS4B3UAx5PRbvANpxviBJkiTVqcaGKAaNaJvcUROPZWAg6ek/umtid19FN8Wy1WtwGPuh9XIo+2rHBlumevoHhta7+/qH9k1WQKu/mFSYSFj6KPAW4K0R0Qn8bWZW7SgaEQ0UgeqtwA7gYxN4riRJknTCaWgI2hqKd41geoLaYECrDFNZ7s+EJIdalzKLd9UGBvcPwJatW0mSU09dMS3lnWzHHZYy83BEvAj4HvDXwOsi4ksUk7s+UJ62DLgQ+F1gJdAFvCgzD0+o1JIkSZKm3NEBbfzauor31VavXDiZxZo2E2lZIjOvj4gnAv8CnAv8aZXTBlvd1gIvy8xbJ/JMSZIkSZoOEwpLAJl5C/DIiHg28DzgfGBJeXgXcAvwrcy8dqLPkiRJkqTpMuGwNKgMQwYiSZIkSbNCQ60LIEmSJEkzkWFJkiRJkqqIzJk3uZWmX0Tsb21tnbdmzZoJ3ae3txeA5ubpnXdAM4v1QNYBgfVA1gHVvg6sX7+e7u7uA5k5/3iuNywJgIjYAXQAWyZ4q8G0tX6C91F9sx7IOiCwHsg6oNrXgdOAw5m5/HguNixpUkXEWoDMPLfWZVHtWA9kHRBYD2QdUP3XAd9ZkiRJkqQqDEuSJEmSVIVhSZIkSZKqMCxJkiRJUhWGJUmSJEmqwtHwJEmSJKkKW5YkSZIkqQrDkiRJkiRVYViSguHcAAAAIABJREFUJEmSpCoMS5IkSZJUhWFJkiRJkqowLEmSJElSFYYlSZIkSarCsCRJkiRJVRiWJEmSJKkKw5IkSZIkVWFYkiRJkqQqDEuSJEmSVIVhSZIkSZKqMCxJkiRJUhWGJUmSJEmqwrAkSZIkSVUYliRJkiSpCsOSJEmSJFVhWJIkSZKkKgxLkiRJklSFYUmSJEmSqjAsSZIkSVIVhiVJkiRJqsKwJEmSJElVGJYkSZIkqQrDkiRJkiRVYViSJEmSpCoMS5IkSZJUhWFJkiRJkqowLEmSJElSFYYlSZIkSarCsCRJkiRJVTTVugCaGSJiB9ABbKl1WSRJkqRJchpwODOXH8/FkZmTXB7Vo4jY39raOm/NmjUTuk9vby8Azc3Nk1Es1SnrgawDAuuBrAOqfR1Yv3493d3dBzJz/vFcb8uSBm1Zs2bNw9euXTuhm2zYsAGA1atXT0aZVKesB7IOCKwHsg6o9nXg3HPP5Y477jjunlO+syRJkiRJVRiWJEmSJKkKw5IkSZIkVWFYkiRJkqQqDEuSJEmSVIWj4UmSJEk6psykp3+AIGgIiDiynK0MS5IkSVIdykx6+5Puvn66egfo6u2ns7efwz39HO7po6tc7+wp9nf2lNsV6919/XT3DRSf3or1vn66eyvW+wbo6RsYsSwR0DAYnoih7cwBGiJobLib2979zLoLVoYlSZIknXAyE5icVpG+/gEOdvdxoKuP/V29HOwq1g90F+vdfQP0DSS9fQP0DiR9/cV2T98AfQMD9PUXoae3v9ju7S+ODQWZ3oGqoaanf4Dya9RcJvRn0l9sDT9KxEDdBSUwLEmSJGkWGBhI9nX2sutQD7sP9bD7UDc7Dw6u97DrUA+7DnYPre851EN/Ji2NDbQ0NtDcNLgMmst9LYP7ho4HXb0DHOju40BXLwe6+jjY1Udnb3+tv/6MV38xqWBYkiRJ0oQMDCRdfUd3+Trc08/6ew/R05f8pnNH0VrS309vX/HeS29/0a2rt3+AnrJVZWi77PLV0z9sOcq+A1199A+Mv5llsLWG7in4hamB9uZG2lsah5YdLY20NRfLymOD+9uaG2ltaqB1cNnUQGtTI63NFetNDbQ1F+vNjQ1EwEAmA1m00GUW24PL4rehWG7esoVMOHXFilr/0hwXw5IkSdIs1j+QQ++f9PQf6dLV1TvA4Z4+Dj/oXZa+odBzuPwU7770DZ1zdCjqo6t35HdZClum5bvOBHNbm5jX1nRk2dZMW1PROtXUWLRaNTcGTQ3Fdku5v6mh2F+c1zAUXFpGCS8tjUf2tzQ10NbUSEPDzGrDaTrUBsDqUxfUuCTHx7AkSZI0Q2Qmnb39xfsuXb3sH3z3pauX/Z1Hun4NLveX60e9qF+GoZ6yxaTvOFpb6llTQ7B4TguL57SwZG4Li+e0smROC0vmtLB4brFcMreVRR0tNDdG2ZJ1dGtXT0Ur15F9xXtErU0NzGtrYn5bM3PbikA0r62ZeW1NzGlponGGhRVNjGFJkiTpOAwMJId7+znc3cehnn4OdRctL4d6+jjcPbgsjh3u6eNQd7ns6T9q/5Fzi+VsyDZtzQ20NzfS3FC8EzS3vZWWsnVlsPWkdWi7+LSU7wQ1V7w/1FLRujK4PXRuUwOtFetzW5tYMqeV+e1NdTmQgGYmw5IkSao7ff0D7D5cvLi/51BvMaLYQNLfn8VyIOkbGCiXeWRZjkI2OBJZZStC91Hb/Q96L6a7b4BDPX109vRzqLu/rl/qH+zi1dHSVLzLUr7b0t7SRMfg+ywtjUPr7cPPG3ofpmno/Zeh92Saj3QF27BhAwCrV6+u5deVjpthSZIkTbmBgaS3DC+9/WV46S+GUe7vP3Ksp2+APWUIKkYyK0cvGzaq2b7O3lp/pSnX0thQdvE60s1rfrmcN7Qs9s1pbRp6AX+wNaZ1+HZFC40tL9LYGJYkSdIx9fYPsK+zl72He9h7uJc9h4+s7+3sYc/hXvYNrh/qZV9nL3sOddPbn/TlnTNmLpip0hAwp7V4Z6WjtbFYlq0wD9pfcXxO67BleXx+WzNtzY21/lrSCc+wJEnSCe5gdx879nVx3/4utg8tO9mxr5sd+4vlzoMzc1zl1qYGFnW00NrcQGND0NQQNDY0lMs4smws9jeX240NcdR7MNXegSmONR451tQwYthpbbK1RpqNDEuSJNWxnr4BDnX3FQMEDA71XDl4wLCBBzp7+jnY3ccDB7rZsa+LHfu6ONDdV+uvQXNjEWCaGxpY0NFcjFxWjlo2uD58dLPFc1roaGk0pEiaMoYlSZJmkJ6+gfJ9ne6hd3Qq13cd6mFXub3rUA8HuqY36DQ3Bgs7WljY3syijhYWdDSzqKO52NfRzML2FhZ1NLOgo5lDu++nuTE48/RVNDUWrT3Fp6Fs6SlGPmsIDDySZiTDkiRJk6yrt59793ay53AP+7v62N85ODdOH/u7eo/MkfOg/X0crFErT1NDcNK8VpYvaCs+89tZvqCV5QvaWT6/jeXz21gyd3wtORtiPwCnL5kzlUWXpCljWJIkaZwOdvexbU8n2/YeZuueTrbt6WTr3nK5p7Nm7/e0NjUcNVBAe0sjc1qL4Z3ntDTS0XpkWOil81o5eX4bpywYDEKtTqYpScMYliRJJ7Su3v6hVp3Blp79Xb3s7+zjQFfv0Pp9+7vYtreTbXs72Xt46oetntPSyOK5LSypeD9n8dwWls5pHVof3L+gvZn25kaaGhumvFySdCIxLEmSZp2+/gHuP9DN9nIAg2Jkty527C+2dx/qKUJQVx89fQNTUoaGgMVzWpjX1sz8cl6c+e1NzGst58tpP3renMHthR1FCHLYaEmqPcOSJKmu9A8kDxzoHuoCd1Qg2t/Njn2dPHCgm4EpntenuTE4dWE7Kxa2s3JROysWdrBi0ZHt5QvaaLalR5LqmmFJkjSjdPX2s31f19A7QcWyq1jfW7QQ9fZPfhJqagjmtz+4FWh+e9H6s3huSxmEOli5qJ1lc1tp8B0fSZrVDEuSpGnR1dvPAwe6uf9AFw8c6C7Xu49a376va1IHR1jU0czyBe3FIAYL2jhlfrFcOq+V+W3NLGgvg1FbM23NTioqSTqaYUmSNCl6+wfYuPMQ63Yc4L/uvJ979/dyOO/jgYPdPLC/e9InPl3Y0cyKhe2curCdUxe0HRWKlpehyPd+JEkTYViSJI1LZrJjfxfrdhzg1zsOsG77ftbtOMCGBw7R0z85gyU0BJw8v40VC9tZsah96N2gFYvaWVkGpDmt/hMmSZpa/ksjSRrRga5e7rrvAOt2HGDd9jIc7djP/q7jbyVaPKeFk+a1smxeK8vmtrJsfrks5/1ZsdDBESRJM4NhSZJEX/8Am3YdGgpF68pQtHVP57ju09gQrF46h5VzG1i1qIVzTj+lCEXzWjlpXhtL5rYYgiRJdcOwJEknkMzkgYPdZfe5I6Ho7vsPjnu+oeXz2zhr+TzOXj6Ps8rPQ06aS2tTIxs2bABg9epVU/E1JEmaFoYlSZqlunr7+c39B7mzfKdo3Y79rNt+gF2HesZ1n7mtTUNh6Ozl8zjr5GJ9YUfLFJVckqSZwbAkSXUuM9m+r4t1O/Zz52Br0fb9bNh5iP5xzMzaELB62VzOWj6Pc5bP46zl8zl7+TxWLmp3SG1J0gnJsCRJdSQz2bK7k1u27uXWLXu5fds+1u04wL7O3nHdZ+ncVs45pWglOvuUIhQ95KS5DrUtSVIFw5IkzWA7D3Zz29a93LJlH7eVAWnP4bEHo5bGBh568lzOXj6fc06Zx9nL53PW8nksm9c6haWWJGl2MCxJ0gxxqLuP27ft49ate7l16z5u3bJ3XKPRnbqgbaiV6OxT5nPO8nmcuXQOTY4+J0nScTEsSVINDAwkG3Ye5KbNe7l5815u3ryHu+47wFhfMVq9dA7nrVzAeSsXcu6p8zl7+XwWdDRPbaElSTrBGJYkaRrsPdzDzVuOBKNbtuzlwBgndl02r5VHrVzI+act4FGnLeS8FQsNRpIkTQPDkiRNsr7+AdbtOFCGoz3csnkvG3YeGtO1c1ubeOSKIhQNhqPl89scjU6SpBowLEnSBN2/v6voTrdlDzdv3suvtu6js7f/mNdFwFknz+OCVQu54LRFnL9qIQ9ZNpeGBoORJEkzgWFJksahu6+ftffuH+pOd/PmvWzbO7ZBGBbPaeHCVQu5YNUiLjhtIY9cuYB5bXankyRppjIsSdIIMpPNuw9z69Z9Q8Hojnv309M/cMxrmxqCh586nwtOK8PRqoWsWtxhdzpJkuqIYWkMIqIdeBvwUmAVsBu4FnhnZm47jvudAbwVeBZwKnAAuBv4ama+b4RrLgf+CHg40AP8DLgyM68f7/MlVXf//i5u3VrOZ1Qu945xTqNTFrQNdae7YNVCHrFigRO8SpJU5wxLxxARbcD3gYuA7cDXgTOAVwHPj4iLMnPDOO73HODLQDtwE0XoWQI8EvhD4EFhKSI+CLwJ6AS+A7QBzwCeGREvzsyvHe/3k05U+7t6+dXWfdyyZS+3bd3LbVv3sX1f15iubW1q4LyVC4a6052/aiGnLGif4hJLkqTpZlg6tndQBKUbgGdm5kGAiHgz8H7gk8AlY7lRRJwNfJWiJekZla1CEdEAXFjlmkspgtIu4AmZeXe5/wnAD4FPRcQPM3PvcX4/adY71N3H2nv386tt+/hVGYzGOjodwBlLOji/7E534apFnH3KPJqd6FWSpFnPsDSKiGgBrig33zAYlAAy86qIeCVwcUQ8OjNvHMMtr6JoFXrR8O5zmTkA/LLKNW8ul1cOBqXy/Bsi4qPAHwOvoQhu0gnvYHcfa7ft41fb9nF7udyw8xA5xslel89v47yV5XxGKxc4p5EkSScww9LongQsANZn5s1Vjn8ZOA/4bWDUsBQRp1G8o7QhM/99LA8v35V6WsWzqj3/j8vnG5Z0wjnQ1cvae/cPhaJfbdvHxnEEowXtzUUwWrlwKCCdPL9tagstSZLqhmFpdI8qlzeNcHxw/3ljuNclQANwfUQ0AZdRhLFG4Hbgi5m5Z9g1ZwGtwAOZuXWCz5fqXmZy9/0H+d6d9/H9O+/nps17GBhjMJrb2sS5p87nkSsWcN5pC3nUygWOTidJkkZlWBrdqnJZLahU7j99DPd6eLk8CFxH8R5Upb8pB2v4wVifn5mHImIvsCgi5mXmgTGUQ6orPX0D/NfGXfznnffzn+vuY8vuY89pNK+1iXNXFMHoESsW8MgVCzhjyRwne5UkSeNiWBrd3HJ5eITjg2+IzxvDvRaVy9dSBKbfpxh+fBnwTuBlwDURcW7FcOTHev5gGRaWZThmWIqItSMcWtPb28uGDWMe2K+qXbt2Teh6zQ4TrQd7O/v4+ZaDXH/PQX659RCHe0ee12hOcwMPXdrGQ5e1cdbSNh62rJ1T5zfTMNRi1A0H7meT/5Uwrfy7QGA9kHVAta8Dvb1jmwJkJIal6TM4dFYT8IeZ+W/l9h7g5RFxFvBYirmU3l6D8kk1tXF3NzdsPsAN9xzkjvs6Ga133UOXtvGEVXN54ulzecjStopgJEmSNHkMS6MbHP2uY4Tjc8rlWP7P+mDF8ktVjn+KIixdPI7nj7cMZOa51fZHxNrm5uaHr169eiy3OabJuo/q27HqwY59XXz9lm1cc/M21u0YuQq3NjXw5Ics5ennnMzTzj6J5QschKFe+HeBwHog64BqVweamyc2oq1haXSby+XKEY4P7r9nDPcaPGdzZtWxujaVy5PG+vyImEPRBW+P7yupXhzq7uPa23dwzc3b+On6nSOOXHfy/FaedvbJXHrOSTxxzVLaWxqnt6CSJOmEZ1ga3a3l8kGTxQ7bf9sY7jU49PiiEY4vLpcHK/b9GugGlkXEiop3mY7n+VLN9A8kP/3NTq65eRvX3r6Dzt7+quc9YsV8Lj3nZC4952TOPXW+I9VJkqSaMiyN7qfAPmBNRJyfmbcMO/7icvnNMdzremAXsDwizsrMXw87Ptj9bmg+p8zsjIjvA88Bfhf44ASeL027O7fv56s3beXrt9zL/Qe6q55z2uJ2Xnj+Cl5wwQpWL5tb9RxJkqRaMCyNIjN7IuKfKAZc+HBEPDMzDwFExJsp5jf6UWYOTUgbEVcAVwDXZObbKu7VFxFXAX9T3uuyzNxfXnMpcDmQwMeGFeMqirD0joj4VmbeXV7zBOAPgb3AJyb/20vH52B3H/922y6+c9c+Nuy+s+o589qaeP55p3LZhSt4zOmLbEGSJEkzUt2FpYh4JcUErl3T9MgrgUuBJwJ3R8R1FPMqPR54AHj1sPOXUkwme0qVe70PeGp5v7si4mfl+RdRTE779sz8eeUFmfm9iPhH4E3ALRHxXaAFeAYQwKsyc+9kfFFpIvr6B/jiL7fwge/exc6DPQ863tQQXHLWSVx24QqedvZJtDX7DpIkSZrZ6i4sUYwa94GI+DxwdWbeeqwLJiIzuyLiqcDbKOZGegGwG/g08M7MHGnC2mr36o2I5wJ/CrwCeBbQA/wI+EBm/r8RrvuTiLiFosXqGeU13wPem5nXH+93kybLD399P3/773dy130HH3TsUact5EUXruD5553K4jktNSidJEnS8anHsHQ18FLgDcAfRcQvgI8D/zrYRW6yZWYn8Ffl51jnvht49yjHe4F/KD/jKcOnKQKaNGP8escB/ubf7+THdz1w1P7WxuCFj1jM6y59BGt8D0mSJNWpugtLmfn6iPhT4PeA1wKPo5if6KqI+AJFa9Mva1lGabZ74EA3V333Lr74i80MVAz9HQGXXbCSl5zdxrK5zQ7YIEmS6lrdhSWAsgXpauDqiHgE8HrgD8rl6yLiNoqBEv7v4CAKkiauq7efT/xkIx/5wW841HP08N8XrV7MO573cB6xYgEbNmyoUQklSZImT12GpUqZeTvwxxHxFuBFwOsohuH+MPC/I+KLwEcqR6yTND4DA8nXb93G+679NffuO3psldVL5/C2557Dpeec5Kh2kiRpVqn7sFShGZhXfqAYKa4ZeBVweURcA7zWkeOk8fn5xt1c+a07uG3rvqP2L+xo5k+e/lD+4KLTaW5sqFHpJEmSpk7dh6WIuIiiNeklQAfQD3wV+ChwHXAZ8JfAC4HDFKPQSRpFZvLju3fysR+t5/r1u4461twYXP7EM7jiqQ9lQUdzjUooSZI09eoyLEXEIuDlFCHp4RStSFuAv6cY4GFHxelfiIgvATcDz53uskr1pLd/gG/dtp2P/mg963YceNDx5z5yOX/57LM5fcmcGpROkiRpetVdWIqIz1G0FrUCCfwHRSvSv2fmQLVrMrOvHGL8ldNWUKmOHO7p44u/2MLV121k297OBx2/cNVC/udzz+ExZyyuQekkSZJqo+7CEsXEsDuATwL/nJmbx3jdNcA9U1YqqQ7tPNjNZ6/fxGd/dg97D/c+6PjTzj6JP3zKah535mIHb5AkSSecegxLvwt8PTP7xnNRZn4T+ObUFEmqL/fsOsTV123k3365he6+oxtkmxqC/3b+qbz+Kas5e/n8GpVQkiSp9uouLGXmV2pdBqke7e/qZd32A3zmhk38x6+2HzWZLMCclkZ+73GrePWTz+TUhe01KaMkSdJMUndhKSIuBF4GfCEzfzHCOY8DXgp8NjNvmc7ySbXS1z/A9n1dbN59+KjPlnJZrZsdwNK5LbzqSWfyssef7uh2kiRJFeouLAFXULy39HejnLMR+COKOZdeNx2FkqZSZrKvs5f79ndz3/4u7tvfxf0Hutm6p3MoDG3b20n/8OaiUZyxpIPXP2UNl124grbmxiksvSRJUn2qx7D0W8BNmfnASCdk5gMRcRNw8fQVSzp+ew/3sPbe/WUQ6i7DUOV6Nz19VQd7HJe25gbOW7GQVz3pDJ557nIaGxy0QZIkaST1GJZWAFW73w1zD3DeFJdFmrA77t3Piz96PYd7+iflfsvnt7FqcQenLe5g1eIOVi1pH9peNrfVUe0kSZLGqB7DUjewcAznzQcm56dPaQp95aatYw5KLU0NnDy/lZPntXHy/DaWL2jj9CVHgtGKhe12qZMkSZok9RiW1gJPjojFmbm72gkRsRh4CnD7tJZMOg4bdx4aWj9jSQfnnDKfk+a1ctL8IhCdPL+1WM5rY357ky1DkiRJ06Qew9LngI8AX46IV2Tm1sqDEbEC+AzQAXy+BuWTxqUyLP3P557DM89dXsPSSJIkaVA9hqWrgd8DLgHuiohrgfXlsTXAs4B24KfAR2tRQGmsevsH2Lz78ND26mVzalgaSZIkVaq7sJSZfRHxHOBDwCuBFww7pR/4FPCmzOyb7vJJ47F1z5HhvhsCTlvcUeMSSZIkaVDdhSWAzDwMvDYi3knRwnRaeWgL8MPM3F6rsknjsXHnwaH1FYvaaW1ycAZJkqSZoi7D0qAyFH2h1uWQjteGB468r3Tm0rk1LIkkSZKGa6h1AaQT2aZdR8LS6qW+ryRJkjST1G3LUkR0AE8FHgrMA6qNp5yZ+d5pLZg0DsOHDZckSdLMUZdhKSIuBz5AMfHs0G4gq2wbljRjbazshrfMbniSJEkzSd11w4uIS4FPUAShvwVuKA/9IfA+4DcUQemfgFfXoozSWHT19nPvvq6hbbvhSZIkzSx1F5aAP6MISk/NzHcCdwNk5scz863AucAHKYLSjTUrpXQMle8rtTQ2cOrC9hqWRpIkScPVY1h6LPCzzLy12sFybqU/B+4H/no6CyaNR2UXvFVLOmhsqPbanSRJkmqlHsPSXGBzxXY3QETMG9yRmQPAfwG/Nb1Fk8Zu467KYcPtgidJkjTT1GNY2gEsrtgenID2YcPOWwzYr0kz1lGDOxiWJEmSZpx6DEvrKIYLH3Q9xYAOfxERARARTwSeBvx6+osnjU3lsOGGJUmSpJmnHsPSt4AzI+Jx5fZ/ArcBLwa2RcSNwA8ovtsHa1NE6dg22Q1PkiRpRqvHsPRZ4DnAfTD0ftLzgO8CJwEXAIeBd2Tm52pVSGk0+zp72XmwZ2jbsCRJkjTz1N2ktJm5D/j2sH3bgGdHRAewALg/M/trUT5pLDZVdMHraGnkpHmtNSyNJEmSqqm7sBQRVwF7MvO9w49l5mGKViVpRhveBa983U6SJEkzSD12w7sCOK/WhZAmYkPFSHhn2AVPkiRpRqrHsLSV+iy3NKRyJLzVhiVJkqQZqR5Dx9eAiysnoZXqjSPhSZIkzXz1GJbeBWwG/j0iLqh1YaTxysyjJqS1G54kSdLMVHcDPABfB7qBJwG/jIjtFOGpq8q5mZlPn87CScey82APB7r7hrbthidJkjQz1WNYuqRiPYBTy081OeWlkcapsgveoo5mFna01LA0kiRJGkk9hqUza10AaSLsgidJklQf6i4sZeY9tS6DNBEbdjq4gyRJUj2oxwEepLq2yWHDJUmS6kLdtSxFxKrxnJ+Zm6eqLNLxqJxjyW54kiRJM1fdhSVgE2MfuCGpz++oWWpgIJ1jSZIkqU7UY5D4MdXDUgNwGrCqXL8B6JnGcknHtH1/F919A0PbZywxLEmSJM1UdReWMvOS0Y5HxMOAqymGFX/OdJRJGqvKkfBOnt/KnNa6+yMoSZJ0wph1Azxk5l3AZcDDgb+ucXGko2zceXBo3S54kiRJM9usC0sAmbkT+C/gpbUui1Rp487DQ+tnLp1bw5JIkiTpWGZlWCoFcHKtCyFVOrplqaOGJZEkSdKxzMqwFBEXABcDTmCrGWXjURPS2rIkSZI0k9Xd2+UR8VejHJ4LPIxiYIcm4GPTUihpDHr7B9iyp3No23eWJEmSZra6C0vAuymGDo9RzjkM/F1mXjUtJZLGYMvuw/QPFKPeNwSsWmw3PEmSpJmsHsPSq0Y51gNsB36RmYdGOU+adpVd8FYu6qClaVb2gpUkSZo16i4sZeZnal0G6Xgc/b6SXfAkSZJmOv9rW5omhiVJkqT6UndhKSIujIirIuKxo5zzuPKc86ezbNJoDEuSJEn1pe7CEnAF8EfAplHO2Vie84bJeGBEtEfEeyLirojoioh7I+KTEbFinPfZFBE5yufsKtd8+hjX/I/J+I6aepsMS5IkSXWl7t5ZAn4LuCkzHxjphMx8ICJuophraUIiog34PnARxeARXwfOoBho4vkRcVFmbhjnbUd672rfKNd8G9hRZf+vx/ls1UBnTz/37usa2jYsSZIkzXz1GJZWAL8Yw3n3AOdNwvPeQRGUbgCemZkHASLizcD7gU8Cl4znhpl5+XGU439l5g+P4zrNAJt2HWlVamls4NSF7TUsjSRJksaiHrvhdQMLx3DefKB/Ig+KiBaKbn8AbxgMSgDlHE63ARdHxKMn8hzNfpVd8E5f0kFjw2jThEmSJGkmqMewtBZ4ckQsHumE8thTgDsm+KwnAQuA9Zl5c5XjXy6Xvz3B52iW21ARls6wC54kSVJdqMdueJ8DPgJ8OSJekZlbKw+Wgy58BugAPj/BZz2qXN40wvHB/ePq7hcRbwHWULSSrQWuGe0drNJlEfEioJFiAItvZua68TxXtVM5Et5qw5IkSVJdqMewdDXwexTvCd0VEdcC68tja4BnAe3AT4GPTvBZq8rl1hGOD+4/fZz3/Ydh2x+IiDdm5idHueaNw7b/PiL+D/CmzOwb5/M1zTbZsiRJklR36i4sZWZfRDwH+BDwSuAFw07pBz7F5ISIueXy8AjHB38CnjfG+30D+AFwI/AAsBp4NfAm4OqI2JWZXx92zc0Ug0t8nyKcLQeeA1xJMTx6D/CnY3w+EbF2hENrent72bBhvAP7HW3Xrl0Tun62+s19+4fW23r2TfjXeaazHsg6ILAeyDqg2teB3t7eCV1fd2EJIDMPA6+NiHdStDCdVh7aAvwwM7fXqmyjycw/HrZrLfBnEbEO+Gfg7ymGJq+85h+HXbMR+EhE/IiiG+APA6zdAAAbx0lEQVQVEXFVZm6ZomJrgg5297O368hYIysXttSwNJIkSRqrugxLg8pQ9IUpfMTg6HcdIxwf7E91YILP+QRFS9FZEXFGZm461gWZuTYivgG8GHg68OmxPCgzz622PyLWNjc3P3z16tVjLvRoJus+s8GtW/YCdwEwp6WRx5z7MCJOjNHwrAeyDgisB7IOqHZ1oLm5eULX191oeBHREBHzI2LEbx4RzeU5E/1+m8vlyhGOD+6/ZyIPycwBjrx3dco4Lr37OK7RNNs47H2lEyUoSZIk1bu6C0sU7+fsAS4e5ZyLy3OGD4owXreWywtHOD64/7YJPgdgUbk8NOpZE79G06wyLJ3p4A6SJEl1ox7D0guBLZn5vZFOKI9tBV40wWf9FNgHrImI86scf3G5/OZEHhIR5wJnUQwkMabhwCOiFXheuTnS0OaaAQxLkiRJ9akew9JDKQZGOJbby3OPW2b2AP9Ubn44IoZ+0o2IN1PMr/SjzLyxYv8VEbEuIv6u8l4R8dyIeNrwZ0TEecCXgACuLp85eOzsiHh5GYwqr1kG/CvFwBa3UoQ6zVCGJUmSpPpUjwM8LKBo7TmWfRzppjYRVwKXAk8E7o6I6yjmVXo8xfDfrx52/lKKVqLh7xE9DnhXRNxDEXAOUwwdfiHF78MPgbcOu2Y58FngHyPil+XzTgUeTTFc+VbgJZmZE/6WmhKZedQcS4YlSZKk+lGPYWk7RYvOsZwH3D/Rh2VmV0Q8FXgb8PsU8zrtphh97p2ZOdKEtcN9m6Il6LHAkyhC337gJ8DngU9lZv+wa+4CPghcBDwSWAJ0l/u/CfxjZu457i+nKbfzYA8Huo9M92VYkiRJqh/1GJa+D1weEf89M79Y7YSIeAnwcOBfJuOBmdkJ/FX5Oda57wbeXWX/DRSTy47nufcyjglnNfNUdsFb1NHMwg7nWJIkSaoX9fjO0vuAHuCzEfFPEXFeRMwpP+dFxD9RhKSe8lypZuyCJ0mSVL/qrmUpM9dFxCuAzwD/X/mpFEAX8KrMvH26yydV2jBsjiVJkiTVj3psWSIzv0TxTtLHgN9QvMfTXa7/H+BRmfnFSZiUVpqQjTsPDq2vNixJkiTVlbprWRqUmb8B/qjasYi4ICKuAl5KMXqcVBObdh4eWj9z6dwalkSSJEnjVbdhabiIOA34A+BlwDkU3fEcUls1MzCQbNxV2Q2vo4alkSRJ0njVdViKiHnA71IEpKdQBKQAtgFfBL5Qu9LpRHfvvk56+gaGts9YYjc8SZKkelJ3YSkiGoFnAy8HfhtoowhIULQkXQJc50StqrXKLnjL57cxp7Xu/rhJkiSd0OpmAISIeGxEfAi4F/gG8BKKsPcNitalXwBk5o8NSpoJKgd3sAueJElS/Znx/9UdEe+geBfpYRxpQboe+Bzwb5m5uzzvT2pTQqm6DUfNseTgDpIkSfVmxocl4D0U3et2AB8BPp+Zm2paImkMKiekddhwSZKk+lMv3fACWA48C3hGRCyscXmkY9rohLSSJEl1rR7C0uOBDwO7gCcDHwW2R8RXIuKyiGiuaemkKnr7B9iyp3No+0zDkiRJUt2Z8WEpM3+RmW+kmFz2d4AvU3TLeyHwJYrg9DHg5NqVUjralt2H6R8oxhlpCFi12AEeJEmS6s2MD0uDMrMvM7+Zmf+dokve64DrgEXl+hqAiPhfEXF+7UoqHd0Fb+WiDlqa6uaPmiRJkkp1+RNcZu7PzE9k5iXAGcDbgXUU7za9BbgxIu6MiHfWrpQ6kW08aiQ8u+BJkiTVo7oMS5Uyc0tm/l1mngs8BvgQcD9wFvDuWpZNJy7DkiRJUv2r+7BUKTNvysw/BVYAzwP+tcZF0gnKsCRJklT/6mGepXHLzAHgP8qPNO0MS5IkSfVvVrUsSTNBZ08/2/d1DW0bliRJkuqTYUmaZJt2HWlVamls4NSF7TUsjSRJko6XYUmaZJVd8E5f0kFjQ9SwNJIkSTpehiVpkvm+kiRJ0uxgWJImmWFJkiRpdjAsSZPMsCRJkjQ7GJakSbbJsCRJkjQrGJakSbTvcC+7DvUMbRuWJEmS6pdhSZpEGyuGDZ/T0siyea01LI0kSZImwrAkTaKNOw8OrZ+5bA4RDhsuSZJUrwxL0iT61m3bh9YfsmxuDUsiSZKkiTIsSZPkN/cf5Ht33j+0/TsXrKhhaSRJkjRRhiVpknziJxuG1h960lwuediyGpZGkiRJE2VYkibBAwe6+cpN24a2X/eU1b6vJEmSVOcMS9Ik+JcbNtHTNwDAsnmt/M75p9a2QJIkSZoww5I0QZ09/Xz2Z/cMbV/+xDNobWqsYYkkSZI0GQxL0gR9+cYt7D3cC0BHSyN/8PhVNS6RJEmSJoNhSZqA/oHk6p9sHNp+yWNOY2FHSw1LJEmSpMliWJIm4Lt37OCeXYcBaAh4zZPPrHGJJEmSNFkMS9IE/POPjwwX/txHnsJpiztqWBpJkiRNJsOSdJxuvGc3N23eO7T9+qesrmFpJEmSNNkMS9JxqmxVevyZizlv5cIalkaSJEmTzbAkHYeNOw/xnTvuG9q2VUmSJGn2MSxJx+ETP9lAZrG+ZtkcnnrWSbUtkCRJkiadYUkap10Hu/nSL7cObb/ut1bT0BA1LJEkSZKmgmFJGqfP/Wwz3X0DACyd28ILLlhR4xJJkiRpKhiWpHHo6u3nszdsGtp+5RPOoK25sWblkSRJ0tQxLEnj8NWbtrHrUA8Abc0NvOyi02tcIkmSJE0Vw5I0RgMDydXXHRku/CWPOY1Fc1pqWCJJkiRNJcOSNEb/ue5+Nuw8BEAEvObJZ9a4RJIkSZpKhiVpjD5eMQnts89dzulL5tSwNJIkSZpqhiVpDG7evIefb9o9tP06J6GVJEma9QxL0hh8vOJdpcecvogLVy2qYWkkSZI0HQxL0jHcs+sQ196+Y2jbViVJkqQTg2FJOoZP/mQjA1msn7l0Dpeec3JtCyRJkqRpYViSRrHnUA//9sutQ9uvefKZNDZEDUskSZKk6WJYkkbx+f+6h87efgAWz2nhxY9eWeMSSZIkabo01boA0nh19vTzvTvv487t++kbSPrLz0AmfQPJwOC+LNezmFC2b2BgqDvdcDnC/p9v3DW0/oonnE5bc+MUfCNJkiTNRIYl1YX+geRnG3Zxzc3buPb2HRzs7pvW57c2NfDyi06f1mdKkiSptgxLYxAR7cDbgJcCq4DdwLXAOzNz2zjuswkY7SfuczJzXZXrGoE/Bl4NPAQ4CPwAeFdm3jnW59ejdTv2c83N2/j6zfeyY39XzcrxiieczpK5rTV7viRJkqafYekYIqIN+D5wEbAd+DpwBvAq4PkRcVFmbhj5DlV9ZoT9+6o8vwH4EvBCYC/wLWAp8GLgeRHx1Mz8+TifP6Pdt7+Lb9xyL1+9eRt3bt9f9Zw5LY089eyTWNjRTGMEDQ1BYwSNjeWyIWgol0OfCCIgovoADSMN23Dy/Daeea4j4EmSJJ1oDEvH9g6KoHQD8MzMPAgQEW8G3g98ErhkPDfMzMvHcfqrKYLS3cBvZeZ95fNfBHwZ+HxEnJOZ09svbZId6u7j22t3cM3N2/jpb3ZWfbeosSF4ykOX8sILV/KMc06mvcX3hyRJkjR1DEujiIgW4Ipy8w2DQQkgM6+KiFcCF0fEozPzxikqxpvL5V8MBqXy+V+JiG8A/w34HeArU/T8KXXz5j189oZ7uPb2HUOjzg133soFvOD8Ffz2o05l2Ty7wkmSJGl6GJZG9yRgAbA+M2+ucvzLwHnAbwOTHpYi4kzgHKCTovtdtef/t/L5dRmW1t5bvJM03IqF7bzgglN54QUreMhJ82pQMkmSJJ3oDEuje1S5vGmE44P7zxvPTSPiLcAaoBtYC1yTmQ+M8vzbM7N3sp4/kzz/vFN4zzfvoKd/gHltTTzvkafwwgtW8NgzFtPg5K+SJEmqIcPS6FaVy60jHB/cP94xpf9h2PYHIuKNmfnJaXr+jLGwo4U/fcbDOH1JB087+yTnMZIkSdKMYVga3dxyeXiE44fK5Vj7iX2DYsjvG4EHgNUUAzi8Cbg6InZl5ten8PlExNoRDq3p7e1lw4bxDux3tF27dh37pGGetSqATu7dcs+Enq2Z43jqgWYX64DAeiDrgGpfB3p7q3XOGjvD0jTKzD8etmst8GcRsQ74Z+DvKYYmlyRJklRjhqXRDY5+1zHC8Tnl8sAEn/MJ4ErgrIg4IzM3TdXzM/PcavsjYm1zc/PDV69ePdZbjWqy7qP6Zj2QdUBgPZB1QLWrA83NzRO6vmGSyjFbbS6XK0c4Prh/Qv3HMnMAWF9unjLdz5ckSZL0YIal0d1aLi8c4fjg/tsm4VmLyuWhin2Dz39ERFSLxZP5fEmSJEkVDEuj+ymwD1gTEedXOf7icvnNiTwkIs4FzqIYyGHd4P7M3AjcCbQDz5uq50uSJEl6sMjMWpdhRouIK4G3A9cDz8zMQ+X+NwPvB36UmZdUnH8FcAXF3Elvq9j/XKArM78/7P7nAf9KMfnshzLzTcOOvxb4OHA38OTMvL/cfxnFRLS/Ac7JzL4Jfs/9ra2t89asWTOR2wyNODLR/qGqb9YDWQcE1gNZB1T7OrB+/Xq6u7sPZOb847neAR6O7UrgUuCJwN0RcR3FvEaPpxj++9XDzl9K0Up0yrD9jwPeFRH3UHSvO0wxdPiFFL8PPwTeWuX5nwSeC7wQWBcR/1k+42KgE3jZRINS6XB3dzd33HHHlgneZzBtrR/1LM121gNZBwTWA1kHVPs6cBojT8NzTIalY8jMroh4KvA24PeBFwC7gU8D78zMkSaMHe7bFL9ZjwWeBCwA9gM/AT4PfCoz+6s8fyAifpdiLqZXA8+neK/pK8C7MvOO4/92Rz1n+WTcZ3Aep5FG3dOJwXog64DAeiDrgOq/DtgNT5Oq3v9AaHJYD2QdEFgPZB1Q/dcBB3iQJEmSpCoMS5IkSZJUhWFJkiRJkqowLEmSJElSFQ7wIEmSJElV2LIkSZIkSVUYliRJkiSpCsOSJEmSJFVhWJIkSZKkKgxLkiRJklSFYUmSJEmSqjAsSZIkSVIVhiVNiohoj4j3RMRdEdEVEfdGxCcjYkWty6bJExGPjoi3RsRXI2JrRGREHHOytoi4PCJ+HhEHI2J3RPx7RDxxOsqsyRMRHRHxgoj4RET8uvyzfigibo2Iv4qIuaNcax2YRSLizeXfA3dHxL6I6I6IeyLisxHxyFGusx7MUhGxJCLuL/9d+M0xzrUezBIR8cPBnwVG+Dx7hOvqpg44Ka0mLCLagB8AFwHbgeuAM4DHAQ8AF2XmhpoVUJMmIr4G/M7w/ZkZo1zzQeBNQCfwHaANeDoQwIsz82tTU1pNtoh4LfDxcvNO4HZgPvBEYB6wDrg4M+8fdp11YJaJiJ3AHOA2YFu5+1zgYUAvcFlm/r9h11gPZrGI+DTwCorfz/WZ+ZARzrMezCIR8UPgYuArwMEqp7w/M3817Jq6qgOGJU1YRFwJvB24AXhmZh4s978ZeD/wo8y8pHYl1GSJiL+k+AHpF+VnE9A6UliKiEuB7wK7gCdk5t3l/icAPwQOA2dm5t4pL7wmLCJeSRGMPpiZd1bsPwX4FnAB8IXM/P2KY9aBWSgingTcmJldw/b/EfBh4D5gZWb2lfutB7NYRDwd+B7wz8DrGSEsWQ9mn4qwdGZmbhrD+XVXB+yGpwmJiBbginLzDYNBCSAzr6L4X8eLI+LRtSifJldm/n1m/lVmfjMzd4zhkjeXyysH/0Is73MD8FFgIfCaKSiqpkBmfiYz/7AyKJX7twNvKDcvK/9eGGQdmIUy86fDg1K5/yPAeuBk4OEVh6wHs1REtAMfA+4A/vcxTrceqO7qgGFJE/UkYAHF/yLdXOX4l8vlb09fkTQTlP+APq3c/HKVU6wbs8ut5bIVWALWgRNYb7nsAevBCeBdwGrgf3Dk9/5BrAeq1zrQVOsCqO49qlzeNMLxwf3nTUNZNLOcRfGD8wOZubXKcevG7LK6XPYCu8t168AJJiJeTvH7fnf5AevBrBUR5wF/BnwqM6+LiDNGOd16MLu9JiKWAAPAXcDXMnPzsHPqsg4YljRRq8pltUpfuf/0aSiLZpZR60ZmHoqIvcCiiJiXmQemr2iaAm8ql9dmZne5bh2Y5SLiLRQDO8wBzinX7wV+LzP7y9OsB7NQRDQAVwN7gb8YwyXWg9ntHcO2/3dEvDcz31uxry7rgN3wNFGDQwUfHuH4oXI5bxrKopnlWHUDrB+zQkQ8l6KPeS/wzopD1oHZ71nAK4EXUwSleyiC0o0V51gPZqc3Ao8F3pKZu8ZwvvVgdvox8HJgDdBB0Xr0dqAPeE9EvKni3LqsA4YlSdJxi4izgc9RDPn6lsy89RiXaBbJzEvL0TAXAU+h6Hr3o4h4e21LpqkUEauAKylGu/10jYujGioHffpcZm7IzM7MvCsz/xZ4QXnKu8t3leqWYUkTNTj6XccIx+eUyxnRlKppday6AdaPuhbFpNPXUvygfFVm/uOwU6wDJ4jM3JuZ1wHPBW4E3hsRjy0PWw9mnw8DLRSDOoyV9eAEkpnfAX5JMbrd48vddVkHfGdJEzX48t7KEY4P7r9nGsqimWXUuhERcyj+Et0zU/ola+wiYjHFZIKnA58C/rzKadaBE0xm9kbEF4FHU4xo9QusB7PR8yneVfpoxFHT7LWVyxXl/DsALy2nmrAenHjuBh4DnFJu12UdMCxpoga73Fw4wvHB/bdNQ1k0s/wa6AaWRcSKzNw27Lh1o05FxFzgPyjm0fkq8LqsPsO5deDEtLNcLiuX1oPZaSHFZKTVtFUcGwxQ1oMTz6JyOfgeUl3WAbvhaaJ+CuwD1kTE+VWOv7hcfnP6iqSZIDM7ge+Xm79b5RTrRh2KiFbg68DjgG9z9KhnR7EOnLAGf0heD9aD2Sgzo9oHOLM8ZX3F/k3lNdaDE0hELAN+q9y8Ceq3DkT1/wyUxi4irqQY+eR64JmZeajc/2bg/RQvgF5SuxJqqkREF9Ba/iNZ7filwHeBXcATBmfrjognAD8AOoEzM3PvNBVZExARjcCXgBcC1wHPzszRRjWyDsxCEfEkipGqvpOZAxX7myneYfkgxf8en5WZW8pj1oMTQDnP0kaKsPSQKsetB7NIRDwROAn4ZuV/mpX14HPAk4BvZObvVByruzpgWNKERUQb8EOKF/i2U/wQdXq5/QBwUWZuqFkBNWki4nkcPTT04yhGQfuvin3vzcxvVVzzQYo5eA5T/AXZAjyjvO7Fmfm1qS63Jkc5BOwHy81rgP0jnPrnmTnYFcs6MMtExOUU76ntpBjMYRewFHgkxbsJXcArM/Pfhl1nPZjljhWWynOsB7NExd8FOyhaj/ZS/Pz3aIrul2uBp2Xm/cOuq6s6YFjSpCiHhXwb8PvAacBuilGy3jnCLM2qQxV/MY7mVcOHki2vu4Ji0soe4GcUoer6yS+lpkpEvBt41xhOPXOw603FtZdjHZgVIuJM4LUU3e1WUwSlHmATRRebD2Xmb0a49nKsB7PWWMJSed7lWA/qXkScQzHf1uMpfvZbRPF+0p0UvRD+T9n1rtq1l1MndcCwJEmSJElVOMCDJEmSJFVhWJIkSZKkKgxLkiRJklSFYUmSJEmSqjAsSZIkSVIVhiVJkiRJqsKwJEmSJElVGJYkSZIkqQrDkiRJkiRVYViSJEmSpCoMS5IkSZJUhWFJknRCiIgcw+fTtS7nsUTEu8uyXl7rskjSbNdU6wJIkjTNPjPKsZ9MWykkSTOeYUmSdELJzMtrXQZJUn2wG54kSZIkVWFYkiRpBOW7QZsioiUi/joi1kdEV0RsiIj3RETbCNctiYj3RcTd5fm7I+LaiHjmKM9aEhF/ExG/iohDEbG/XP+HiDhlhGseGRHfiIg95TU/iognjnDucyPiuxGxLSK6I+LeiPhJRLzr+H51JGn2i8ysdRkkSZpyEZEAmRnjvGYzcBvwdOA/gZ5yfUG5/azM7K+4ZgXwY2B1ee0NwDLgYqAReHNmfmDYc84BvgOsBHaU1wA8DDgXeGFmfq08993Au4APA68C1gN3AGcDjwK6gMdm5u0V938D8E9AP/BTYBuwFDgHWDmeXxNJOpH4zpIkSaNbRdET4xGZuQEgIpYB36cITW8EPlhx/kcpgtL/BV6VmT3lNU8Gvg28LyJ+kJm3lPubgGsogtIHgb8cvKY8fi5FABruDcCbMvNDFed+APgT4C+AV1Sc+xdAAhdl5i8rzg+KECdJqsJueJKkE8oxhg5/wQiXvWcwKAFk5gPAW8rNKyruvRp4PnAQeGNl6MnMn1AEqUaKoDPoMuAsYC3w55XXlNetzcz1Vcr008qgVLqyXD5l2P5lwN7KoFTeOzPzh1XuLUnCliVJ0olntKHDN4+w/1+H78jMayNiD7AmIk7JzO3Ak8vD12bm7ir3+RfgzcBvVey7tFxeXdmdbwy+U6VMuyJiNzD8HacbgSdHxCeAqzJz7TieI0knLMOSJOmEchxDh+/JzAMjHLsHWAScCmwvlwCbRjh/cP+Kin2nlctqrUej2TrC/gPA4mH73gB8DXg18OqIuA/4EfBV4MvjDGmSdMKwG54kSdNnMkdVGhjzQzNvAx4OvBD4OLAfeAlFi9l1EdEyieWSpFnDsCRJ0ugWRcS8EY6tKpf3DluePsL5Z5TLbRX7tpTLNcdVujHKzK7M/Fpmvj4zHwY8gmKUvycAr53KZ0tSvTIsSZJ0bC8ZvqOcM2kxsKF8XwngJ+Xy2RGxsMp9XlYur6vY971y+ZqImLZ/l8v3lj5cbj5iup4rSfXEsCRJ0rG9KyLOGNyI+P/bu1cWq6IwDMDvF5wgCAZ/gMUkOGmCSdEiBlE0TdAolimTLDaDtzCgRQz+CZMMVotB0GIQL/gPRFAHlmHt0fG4HHAQhTnP0/bl25wTX/Ze76oDSW5Nh5uBI1Nj3qMk+5KsVdWeLTNHk1xJ3+vo+0z6uqFX6YHl5taZae7w1LK3I1W1t6pWZsPbFMxOTYfvf50EQMEDAHOlqh5uc/lda+3a7Ln0z9VeVtV6kq9JTiTZn+RJktn67svpb44uJjlWVZub0h5Prw1f3dxjKUlaaxtVdT7J4ySrSZanmUpyKD1EnUvyOjuzkGQtye2qepZeMrGQZCm9XOJNkvs7fDbAriYsATBvLm1z7XmS2bDUklyYzi/nR/PdvSTXW2sbP93c2oeqWkpyNcnZ9H2UPiVZT3KntTaq/H5RVYvpezedSXI6yef0oHYjydM//I9bfUxvwzuZZDHJkSRfpmc/SHL3NzXnAHOvWvubxTwAsHtUVUvytrV28H//FgD+PWuWAAAABoQlAACAAWEJAABgwJolAACAAW+WAAAABoQlAACAAWEJAABgQFgCAAAYEJYAAAAGhCUAAIABYQkAAGBAWAIAABgQlgAAAAaEJQAAgAFhCQAAYEBYAgAAGBCWAAAABoQlAACAgW+F7WzJ5FRhqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": [],
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMoDbTHYFROP"
   },
   "source": [
    "# 4 - Operations on Embeddings\n",
    "\n",
    "Next, let's retrieve the word embeddings learned during training. This will be a matrix of shape `(vocab_size, embedding_size)`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nL4LDEV7ELZn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d670919-8e83-4d68-8a9c-dd7cebbbc88f"
   },
   "source": [
    "emb_matrix = gru_model.layers[1].get_weights()[0]\n",
    "print(emb_matrix.shape)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl1F4LvdaWMv"
   },
   "source": [
    "Using the word indices as defined by the tokenizer, you can now retrieve the word embedding vectors from the embedding matrix.\n",
    "\n",
    "**Task**: Complete the function `get_embedding_vector` for returning the embedding vector of a given word `word_in`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_hkJKYavWqPj"
   },
   "source": [
    "def get_embedding_vector( word_in, tokenizer, embedding_matrix ):\n",
    "  \n",
    "  assert word_in in tokenizer.word_index.keys(), 'word not in vocabulary: \"{}\"'.format(word_in)\n",
    "\n",
    "  ### START YOUR CODE HERE ###  (≈2 LOC)\n",
    "\n",
    "  # Get the word index\n",
    "  ind = tokenizer.word_index[word_in]\n",
    "  # Lookup the embedding vector\n",
    "  emb_vec = emb_matrix[ind]\n",
    "\n",
    "  \n",
    "  ### END YOUR CODE HERE ###\n",
    "\n",
    "  return emb_vec"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qoQzFDEMTDvA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0d707287-2f03-4137-8318-41bd12707b0a"
   },
   "source": [
    "len(list(tokenizer.index_word.keys()))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16286"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iD0SU1FwD7k3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "864507a2-4131-4305-ca93-de19fa13e875"
   },
   "source": [
    "get_embedding_vector('dog', tokenizer, emb_matrix)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.01588162,  0.04801065, -0.00946326,  0.02618463, -0.03257658,\n",
       "        0.02487237,  0.0173738 , -0.0404321 , -0.03775306,  0.04921504,\n",
       "       -0.04428474, -0.04773955, -0.03934409, -0.04432237,  0.00366922,\n",
       "        0.04565248], dtype=float32)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeKKqlWxViqP"
   },
   "source": [
    "## 4.1 - Word Similarity\n",
    "\n",
    "To measure how similar two words are, we need a way to measure the degree of similarity between two embeddings vectors for the two words. Given two word vectors $u$ and $v$, cosine similarity is defined by the cosine of the angle $\\theta$ between the two vectors: \n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = cos(\\theta) = \\frac {u \\cdot v} {\\Vert u \\Vert \\Vert v \\Vert} $$\n",
    "\n",
    "where $u \\cdot v$ is the dot product of two vectors, $\\Vert u \\Vert$ is the norm (or length) of the vector $u$, and $\\theta$ is the angle between $u$ and $v$.\n",
    "\n",
    "If $u$ and $v$ are very similar, their cosine similarity will be close to 1. If they are dissimilar, the cosine similarity will take a smaller value down to -1. \n",
    "\n",
    "**Note**: The norm of $u$ is defined as $ \\Vert u \\Vert = \\sqrt{\\sum_{i=1}^{n} u_i^2}$.\n",
    "\n",
    "**Task**: Complete the function `cosine_similarity`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YTVMBBWHVi8q"
   },
   "source": [
    "def cosine_similarity(u, v):\n",
    "\n",
    "  ### START YOUR CODE HERE ###  (≈3 LOC) \n",
    "  norm_a = np.linalg.norm(u)\n",
    "  norm_b = np.linalg.norm(v)\n",
    "  cos_similarity = np.dot(u, v) / (norm_a * norm_b)\n",
    "  ### END YOUR CODE HERE ###\n",
    "\n",
    "  return cos_similarity"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pd0qIsvVWZa"
   },
   "source": [
    "Let's test some combinations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cr5i2ucUsnH4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "516fdf1b-0074-4769-8f70-081b392d849e"
   },
   "source": [
    "print(cosine_similarity(get_embedding_vector( 'beautiful', tokenizer, emb_matrix ),\n",
    "                        get_embedding_vector( 'wonderful', tokenizer, emb_matrix )))\n",
    "\n",
    "print(cosine_similarity(get_embedding_vector( 'woman', tokenizer, emb_matrix ),\n",
    "                        get_embedding_vector( 'man', tokenizer, emb_matrix )))"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "0.896077\n",
      "0.12309178\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DeJDhIbVZsN"
   },
   "source": [
    "## 4.2 - Similar Word Retrieval\n",
    "\n",
    "As mentioned in the video, we can use the word embedding for retrieval of $k$ semantically close words:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J2LNV8PqFNJ1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cb6d5d27-4de9-4282-8d2a-decf90743b75"
   },
   "source": [
    "def closest_word(word_in, emb_matrix, top_k=5):\n",
    "\n",
    "  # Get embedding vector of input word\n",
    "  word_in_emb = get_embedding_vector(word_in, tokenizer, emb_matrix)\n",
    "\n",
    "  # Compute similarities\n",
    "  similarity = [ cosine_similarity(w_emb, word_in_emb) for w_emb in emb_matrix ]\n",
    "\n",
    "  # Top-k words having largest similarity\n",
    "  idxs = np.argsort( similarity )[::-1][1:top_k+1]\n",
    "\n",
    "  return [ [tokenizer.index_word[i], similarity[i]] for i in idxs ]\n",
    "\n",
    "\n",
    "for word in ['beautiful', 'news']:\n",
    "  print(closest_word(word, emb_matrix))"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[['important', 0.9083743], ['wonderful', 0.896077], ['every', 0.88394386], ['potential', 0.8737282], ['good', 0.8698812]]\n",
      "[['credibility', 0.87429506], ['soon', 0.81251645], ['trump', 0.80276686], ['word', 0.7987612], ['happy', 0.78762835]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml9h-Um8VjwR"
   },
   "source": [
    "## 4.3 - Word Analogy\n",
    "\n",
    "Another interesting task is the \"Word analogy task\", where we complete the sentence \"`a` is to `b` as `c` is to __\". In detail, we are trying to find a word `d`, such that the associated word vectors $e_a$, $e_b$, $e_c$, $e_d$, are related as follows:\n",
    "$$ e_b - e_a \\approx e_d - e_c. $$ \n",
    "\n",
    "The similary between $ e_b - e_a $ and $ e_d - e_c $ is measured using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1ctEG9OSGZ5U"
   },
   "source": [
    "def complete_analogy(word_a, word_b, word_c):\n",
    "\n",
    "  # Convert words to lower case\n",
    "  word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "\n",
    "  # Get the embedding vectors\n",
    "  e_a = get_embedding_vector(word_a, tokenizer, emb_matrix)\n",
    "  e_b = get_embedding_vector(word_b, tokenizer, emb_matrix)\n",
    "  e_c = get_embedding_vector(word_c, tokenizer, emb_matrix)\n",
    "  \n",
    "  max_cosine_sim = -100\n",
    "  best_word = None\n",
    "\n",
    "  # Loop over the whole word vector set\n",
    "  for w_idx, w in enumerate(emb_matrix):\n",
    "    \n",
    "    # To avoid best_word being one of the input words, pass on them.\n",
    "    if (w == [e_a, e_b, e_c]).all(1).any():\n",
    "      continue\n",
    "    \n",
    "    # Compute cosine similarity between the vector (e_b - e_a) and the vector (w - e_c)\n",
    "    ### START YOUR CODE HERE ###  (≈1 LOC)\n",
    "    cosine_sim = cosine_similarity((e_b - e_a), (w - e_c) )\n",
    "    ### END YOUR CODE HERE ###\n",
    "\n",
    "    if cosine_sim > max_cosine_sim:\n",
    "      # Set new max similarity\n",
    "      max_cosine_sim = cosine_sim\n",
    "      # Select new best_word\n",
    "      best_word_idx = w_idx\n",
    "\n",
    "      print(cosine_sim, tokenizer.index_word[best_word_idx])\n",
    "      \n",
    "  return tokenizer.index_word[best_word_idx]"
   ],
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u-JolLb-GbAY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f94bd647-9d7e-4a3d-ef56-77f113dcbf50"
   },
   "source": [
    "print(complete_analogy('china', 'beijing', 'usa'))\n",
    "\n",
    "print(complete_analogy('man', 'king', 'woman'))"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "0.6917039 <pad>\n",
      "<pad>\n",
      "0.47493193 <pad>\n",
      "0.5253788 prior\n",
      "0.56955963 broken\n",
      "0.61883724 popular\n",
      "0.6264749 fracking\n",
      "0.6273687 sides\n",
      "0.6869686 touch\n",
      "0.71308845 normal\n",
      "0.75096744 holocaust\n",
      "0.7884558 balance\n",
      "balance\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak5nkmWtmGxu"
   },
   "source": [
    "You may try different word combinations. However, our word embedding is not very powerful as we used a rather simple model for training, and more importantly, a comparably small text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4nP9FYhcpdf"
   },
   "source": [
    "## 4.4 - GloVe Embedding\n",
    "\n",
    "Execute the cell below to download a more powerful word embedding, i.e., a 50-dimensional GloVe word embedding trained on Wikipedia (2014) and the Gigaword 5 corpus. The [GloVe embedding](https://nlp.stanford.edu/projects/glove/) is provided by the NLP research group at Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HuSkrbHVmu7T",
    "cellView": "form"
   },
   "source": [
    "#@title Download GloVe Embedding\n",
    "\n",
    "import requests, os, zipfile\n",
    "import numpy as np\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def unzip(file, destination=None):\n",
    "  if not destination or not os.path.isdir(destination):\n",
    "    destination = os.path.dirname(file)\n",
    "  print('unzipping to ',destination)\n",
    "  zip_ref = zipfile.ZipFile(file, 'r')\n",
    "  zip_ref.extractall(destination)\n",
    "  zip_ref.close()\n",
    "\n",
    "\n",
    "data_path = '/tmp/glove'\n",
    "glove_file = os.path.join(data_path, 'glove.6B.50d.txt')\n",
    "!rm -rf $data_path\n",
    "os.makedirs(data_path)\n",
    "\n",
    "# download glove file\n",
    "download_file_from_google_drive('1JUdsQpf_-fGs8kYA4PBS1rnd0IPZYFVd', glove_file)\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "  with open(glove_file, 'r') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    \n",
    "    for line in f:\n",
    "      line = line.strip().split()\n",
    "      curr_word = line[0]\n",
    "      words.add(curr_word)\n",
    "      word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "          \n",
    "  return words, word_to_vec_map"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HHmQji_yqbCH"
   },
   "source": [
    "words, word_to_vec_map = read_glove_vecs(glove_file)"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tvrwofYs698"
   },
   "source": [
    "You can get the embedding vector of a string by a lookup in the `word_to_vec_map`. Let's compute some word similarities using the GloVe embedding:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SWABnyJgtCSN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "96c406bc-2f53-4b30-b046-c7a44aa92fda"
   },
   "source": [
    "print(cosine_similarity(word_to_vec_map['beautiful'],\n",
    "                        word_to_vec_map['wonderful'],))\n",
    "\n",
    "print(cosine_similarity(word_to_vec_map['dog'],\n",
    "                        word_to_vec_map['cat'],))\n",
    "\n",
    "print(cosine_similarity(word_to_vec_map['woman'],\n",
    "                        word_to_vec_map['man'],))"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "0.8296378712614463\n",
      "0.9218005273769252\n",
      "0.886033771849582\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRthN117e91L"
   },
   "source": [
    "Let's see if the GloVe Embedding allows for better analogies:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AEm6ZQzyZ-Qb"
   },
   "source": [
    "def complete_analogy_glove(word_a, word_b, word_c):\n",
    "\n",
    "  # Convert words to lower case\n",
    "  word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "\n",
    "  # Get the embedding vectors\n",
    "  e_a = word_to_vec_map[word_a]\n",
    "  e_b = word_to_vec_map[word_b]\n",
    "  e_c = word_to_vec_map[word_c]\n",
    "  \n",
    "  words = word_to_vec_map.keys()\n",
    "  max_cosine_sim = -100\n",
    "  best_word = None\n",
    "\n",
    "  # Loop over the whole word vector set\n",
    "  for w in words:        \n",
    "    \n",
    "    # To avoid best_word being one of the input words, pass on them.\n",
    "    if w in [word_a, word_b, word_c] :\n",
    "      continue\n",
    "    \n",
    "    # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)\n",
    "    cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "    \n",
    "    if cosine_sim > max_cosine_sim:\n",
    "      # Set new max similarity\n",
    "      max_cosine_sim = cosine_sim\n",
    "      # Select new best_word\n",
    "      best_word = w\n",
    "      \n",
    "  return best_word"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jve-Ak7Nu6_K"
   },
   "source": [
    "Try out a few! Cool triplets are\n",
    "- `('king', 'man', 'queen')`\n",
    "- `('russia', 'russian', 'china')`\n",
    "- `('india', 'delhi', 'japan')`\n",
    "- `('man', 'woman', 'boy')`\n",
    "- `('small', 'smaller', 'big')`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fRdX57Rshp3N",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "7a26a0ac-c97d-4f0e-8837-b2d376a74686"
   },
   "source": [
    "complete_analogy_glove('man', 'woman', 'boy')"
   ],
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'girl'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 39
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrIxnff2mTGi"
   },
   "source": [
    "And for the most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D-frwnAow2jP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "69ca0d47-8307-4016-fd10-23d37be8661c"
   },
   "source": [
    "def closest_word_glove(embedding_vector, remove_words=[], top_k=5):\n",
    "\n",
    "  # Get vocabulary\n",
    "  vocabulary = list(word_to_vec_map.keys())\n",
    "  # To avoid top words being one of the input words, remove them from list\n",
    "  for w in remove_words:\n",
    "    vocabulary.remove(w)\n",
    "\n",
    "  # Compute embeddings of all words\n",
    "  w_embeddings = np.array([word_to_vec_map[w] for w in vocabulary])\n",
    "\n",
    "  # Compute similarities\n",
    "  similarity = [ cosine_similarity(w_emb, embedding_vector) for w_emb in w_embeddings ]\n",
    "\n",
    "  # Index of max similary\n",
    "  idxs = np.argsort( similarity )[::-1][:top_k]\n",
    "\n",
    "  return [ [vocabulary[i], similarity[i]] for i in idxs ]\n",
    "\n",
    "\n",
    "for word in ['beautiful', 'news']:\n",
    "  print(closest_word_glove(word_to_vec_map[word], [word]))"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[['lovely', 0.921088084676833], ['gorgeous', 0.893486407216522], ['wonderful', 0.8296378712614463], ['charming', 0.8249218663752111], ['beauty', 0.8014683990617305]]\n",
      "[['press', 0.8883447788255655], ['interview', 0.849574463576904], ['newspaper', 0.8341822594978635], ['newspapers', 0.828009445659333], ['reported', 0.8237055907435875]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sbNGZ5xdbxK"
   },
   "source": [
    "***\n",
    "\n",
    "# Congratulations!\n",
    "\n",
    "You may now submit your notebook to moodle:\n",
    "- Enter your email adress in the cell below.\n",
    "- Save the notebook (`CTRL`+ `s` or '*File*' -> '*Save*')\n",
    "- Click on '*File*' -> '*Download .ipynb*' for downloading the notebook as IPython Notebook file.\n",
    "- Upload the downloaded IPython Notebook file to **Moodle**."
   ]
  }
 ]
}